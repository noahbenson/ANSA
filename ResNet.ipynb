{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Naming Convention\n",
    "\n",
    "Each `.pt` file should follow this format:\n",
    "\n",
    "```\n",
    "<label>_<replicate>.pt\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "- `150_1.pt` → Label: 150 (undetectable), Replicate: 1\n",
    "- `500_2.pt` → Label: 500 (low), Replicate: 2\n",
    "- `7000_3.pt` → Label: 7000 (medium), Replicate: 3\n",
    "- `20000_5.pt` → Label: 20000 (high), Replicate: 5\n",
    "\n",
    "\n",
    "\n",
    "## Class Definitions for Semi-Quantitative approach \n",
    "\n",
    "Infers clinical decision-making based on viral load counts (assuming 1:1 sample prep\n",
    ")\n",
    "1. `undetectable` → Label values `< 200`\n",
    "2. `low` → Label values `200 ≤ label ≤ 1000`\n",
    "3. `medium` → Label values `1000 < label ≤ 10000`\n",
    "4. `high` → Label values `> 10000`\n",
    "\n",
    "# Dataset Folder Structure\n",
    "\n",
    "Dataset is organized into the following structure to ensure proper training, validation, and testing:\n",
    "\n",
    "```\n",
    "Datasets/\n",
    "│-- SemiQuant/\n",
    "│   │-- Training/             # Training dataset (60% of total data)\n",
    "│   │   ├── undetectable/      # Class 0 (e.g., files with labels < 200)\n",
    "│   │   │   ├── 20_1.pt\n",
    "│   │   │   ├── 40_3.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── low/               # Class 1 (200 ≤ label ≤ 1000)\n",
    "|   |   |   ├── 300_2.pt\n",
    "│   │   │   ├── 600_4.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── medium/            # Class 2 (1000 < label ≤ 10000)\n",
    "│   │   │   ├── 2000_1.pt\n",
    "│   │   │   ├── 7000_2.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── high/              # Class 3 (label > 10000)\n",
    "│   │   │   ├── 10000_2.pt\n",
    "│   │   │   ├── 90000_2.pt\n",
    "│   │   │   └── ...\n",
    "│\n",
    "│   │-- Validation/            # Validation dataset (20% of total data)\n",
    "│   │   ├── undetectable/\n",
    "│   │   │   ├── 20_2.pt\n",
    "│   │   │   ├── 40_4.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── low/\n",
    "│   │   ├── medium/\n",
    "│   │   ├── high/\n",
    "│\n",
    "│   │-- Testing/               # Testing dataset (20% of total data)\n",
    "│   │   ├── undetectable/\n",
    "│   │   │   ├── 30_1.pt\n",
    "│   │   │   ├── 50_2.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── low/\n",
    "│   │   ├── medium/\n",
    "│   │   ├── high/\n",
    "│\n",
    "│-- torch_tensors/              # Original .pt files before splitting\n",
    "│   │   ├── 100_1.pt\n",
    "│   │   ├── 200_3.pt\n",
    "│   │   ├── 5000_2.pt\n",
    "│   │   ├── 15000_4.pt\n",
    "│   │   └── ...\n",
    "```\n",
    "\n",
    "## Folder Descriptions\n",
    "\n",
    "- **`Training/`** – Used to train the model (60% of total data).\n",
    "- **`Validation/`** – Used to validate the model during training (20% of total data).\n",
    "- **`Testing/`** – Used to evaluate the model after training (20% of total data).\n",
    "- **`torch_tensors/`** – Stores the original `.pt` files before they were split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTDataset(Dataset):\n",
    "    def __init__(self, root_dir, target_size=(500, 500), transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the dataset directory (e.g., Training folder).\n",
    "            target_size (tuple): Desired output size (height, width).\n",
    "            transform (callable, optional): Optional transformations (on CPU).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "        self.classes = ['undetectable', 'low', 'medium', 'high']\n",
    "\n",
    "        # Collect all file paths and labels\n",
    "        self.file_list = []\n",
    "        for label in self.classes:\n",
    "            class_path = os.path.join(root_dir, label)\n",
    "            if not os.path.exists(class_path):\n",
    "                continue  # Skip if folder doesn't exist\n",
    "            for file in os.listdir(class_path):\n",
    "                if file.endswith('.pt'):\n",
    "                    full_path = os.path.join(class_path, file)\n",
    "                    class_index = self.classes.index(label)\n",
    "                    self.file_list.append((full_path, class_index))\n",
    "\n",
    "        # Pre-load everything into memory (CPU)\n",
    "        self.data_list = []\n",
    "        for file_path, label in self.file_list:\n",
    "            # Load from disk to CPU memory\n",
    "            tensor_data = torch.load(file_path, map_location='cpu')  # [C, T, H, W]\n",
    "\n",
    "            # Ensure enough frames\n",
    "            max_frames = tensor_data.shape[1]\n",
    "            # selected_frame_indices = [ 49, 59, 69, 79, 89, 99, 109, 119, 129, 139, 149, 159, 169, 179]\n",
    "            selected_frame_indices = [69, 89, 109, 129, 149, 179]\n",
    "            selected_frame_indices = [i for i in selected_frame_indices if i < max_frames]\n",
    "            if len(selected_frame_indices) < 6:\n",
    "                raise ValueError(f\"Not enough frames in {file_path}, available: {max_frames}, required: 180\")\n",
    "\n",
    "            # Compute average of the first 20 frames\n",
    "            avg_first_20 = torch.mean(tensor_data[:, :20, :, :], dim=1, keepdim=True)  # [C, 1, H, W]\n",
    "            selected_frames = tensor_data[:, selected_frame_indices, :, :]             # [C, 6, H, W]\n",
    "\n",
    "            # Concatenate to form a 7-frame tensor\n",
    "            final_tensor = torch.cat((avg_first_20, selected_frames), dim=1)  # [C, 7, H, W]\n",
    "            final_tensor = final_tensor.squeeze(0) if final_tensor.shape[0] == 1 else final_tensor\n",
    "\n",
    "            # Resize on CPU\n",
    "            if final_tensor.dim() == 3:\n",
    "                # shape [7, H, W]\n",
    "                final_tensor = final_tensor.unsqueeze(0)  # -> [1, 7, H, W]\n",
    "\n",
    "            resized_tensor = F.interpolate(\n",
    "                final_tensor,\n",
    "                size=self.target_size,\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            # Optional transform\n",
    "            if self.transform:\n",
    "                resized_tensor = self.transform(resized_tensor)\n",
    "\n",
    "            # Model expects input_channels=7, flatten [C=1, frames=7, H, W] -> [7, H, W]\n",
    "            if resized_tensor.shape[0] == 1:\n",
    "                resized_tensor = resized_tensor.squeeze(0)  # shape [7, H, W]\n",
    "\n",
    "            # Store (tensor, label)\n",
    "            self.data_list.append((resized_tensor, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_model(num_classes=4, input_channels=7, dropout_rate=0.1454303215712593):\n",
    "    \"\"\"\n",
    "    Build ResNet18 with a custom first conv layer\n",
    "    that expects `input_channels` and adds a Dropout layer.\n",
    "\n",
    "    model_depth = 18 (ResNet18)\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    \n",
    "    # Replace first conv to match your input_channels\n",
    "    model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    \n",
    "    # Replace FC layer to include Dropout before classification\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_rate),  # Dropout before final classification\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            # Use AMP if on GPU\n",
    "            with torch.amp.autocast(device_type='cuda', enabled=(device.type == 'cuda')):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=18):\n",
    "    \"\"\"\n",
    "    Basic training routine using CrossEntropyLoss\n",
    "    for single-label, multi-class classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            with torch.amp.autocast(device_type='cuda', enabled=(device.type == 'cuda')):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = 100.0 * correct / total\n",
    "\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "def plot_confusion_matrix(model, loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Generates and displays a confusion matrix for the model on the given loader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train_dataset_path = 'H:/Datasets/int_split/Training/'\n",
    "val_dataset_path   = 'H:/Datasets/int_split/Validation/'\n",
    "test_dataset_path  = 'H:/Datasets/int_split/Testing/'\n",
    "\n",
    "def main():\n",
    "    #Check device for CUDA or CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    \n",
    "    train_dataset = PTDataset(root_dir=train_dataset_path, target_size=(500, 500))\n",
    "    val_dataset   = PTDataset(root_dir=val_dataset_path,   target_size=(500, 500))\n",
    "    test_dataset  = PTDataset(root_dir=test_dataset_path,  target_size=(500, 500))\n",
    "\n",
    "   \n",
    "    train_labels = [label for _, label in train_dataset.data_list]\n",
    "    class_counts = Counter(train_labels)\n",
    "    weights = [1.0 / class_counts[label] for label in train_labels]\n",
    "    \n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=weights,\n",
    "        num_samples=len(weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    use_pin_memory = (device.type == 'cuda')\n",
    "    batch_size = 32 \n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    \n",
    "    best_val_acc = -float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    # Train/Evaluate 5 times\n",
    "    for run_idx in range(5):\n",
    "        print(f\"\\n=== Training Run {run_idx+1} of 5 ===\")\n",
    "\n",
    "        \n",
    "        model = get_resnet_model(num_classes=4, input_channels=7)  \n",
    "        model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        learning_rate = 0.0038825206157311557\n",
    "        weight_decay  = 0.0001931053552153856\n",
    "        gamma_rate    = 0.9388047294838997\n",
    "        \n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            momentum=0.9\n",
    "        )\n",
    "\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=gamma_rate)\n",
    "\n",
    "        \n",
    "        num_epochs = 25\n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            num_epochs=num_epochs\n",
    "        )\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        print(f\"Run {run_idx+1} validation accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        # Keep track of best model so far\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"New best model found with val_acc={val_acc:.2f}% (Run {run_idx+1}).\")\n",
    "\n",
    "    # After all 5 runs, save only the best model\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, 'resnet_model_optuna.pth')\n",
    "        print(f\"\\nBest model saved with val_acc={best_val_acc:.2f}%\")\n",
    "\n",
    "        best_model = get_resnet_model(num_classes=4, input_channels=7)\n",
    "        best_model.load_state_dict(best_model_state)\n",
    "        best_model.to(device)\n",
    "\n",
    "        test_loss, test_acc = evaluate_model(best_model, test_loader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}% for the best model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def load_best_model(model_path, num_classes=4, input_channels=7, dropout_rate=0.1454303215712593):\n",
    "    \"\"\"\n",
    "    Load the best trained model from saved weights.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    model.conv1 = torch.nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.fc = torch.nn.Sequential(\n",
    "        torch.nn.Dropout(p=dropout_rate),\n",
    "        torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()  \n",
    "\n",
    "    return model, device\n",
    "\n",
    "def plot_confusion_matrix(model, loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Generates and displays a confusion matrix for the model on the given loader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "model_path = \"resnet_model_optuna.pth\"\n",
    "model, device = load_best_model(model_path)\n",
    "\n",
    "test_dataset_path = 'H:/Datasets/int_split/Testing/'\n",
    "test_dataset = PTDataset(root_dir=test_dataset_path, target_size=(500, 500))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=(device.type == 'cuda')\n",
    ")\n",
    "\n",
    "class_names = ['Und.', 'Low', 'Med', 'High']  \n",
    "\n",
    "plot_confusion_matrix(model, test_loader, device, class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "def hyperparameter_search(output_dir, hyperparams, train_dataset, val_dataset, test_dataset):\n",
    "    \"\"\"\n",
    "    Perform a grid search over the given hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): Directory to save results\n",
    "        hyperparams (dict): Dictionary of hyperparameters to search\n",
    "        train_dataset, val_dataset, test_dataset: Datasets for training, validation, and testing\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Get all hyperparameter combinations\n",
    "    keys, values = zip(*hyperparams.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    \n",
    "    for params in param_combinations:\n",
    "        model_depth = params['model_depth']\n",
    "        batch_size = params['batch_size']\n",
    "        learning_rate = params['learning_rate']\n",
    "        weight_decay = params['weight_decay']\n",
    "        dropout = params['dropout']\n",
    "        gamma = params['gamma']\n",
    "        gamma_rate = params['gamma_rate']\n",
    "        num_epochs = params['num_epochs']\n",
    "        optimizer_type = params['optimizer']\n",
    "        \n",
    "        # Weighted sampler for balancing dataset\n",
    "        train_labels = [label for _, label in train_dataset.data_list]\n",
    "        class_counts = Counter(train_labels)\n",
    "        weights = [1.0 / class_counts[label] for label in train_labels]\n",
    "        train_sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "        \n",
    "        # DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = get_resnet_model(model_depth=model_depth, dropout=dropout).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        if optimizer_type == 'Adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'SGD':\n",
    "            optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)\n",
    "        elif optimizer_type == 'RMSprop':\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer type\")\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma_rate) if gamma else None\n",
    "        \n",
    "        logs = []\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss, correct, total = 0.0, 0, 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "            \n",
    "            train_loss = running_loss / total\n",
    "            train_acc = 100.0 * correct / total\n",
    "            val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            logs.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc\n",
    "            })\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Save results\n",
    "        result_dir = os.path.join(output_dir, f\"model_{model_depth}_batch_{batch_size}_lr_{learning_rate}_wd_{weight_decay}_dropout_{dropout}_gamma_{gamma}_gammaRate_{gamma_rate}_epochs_{num_epochs}_opt_{optimizer_type}\")\n",
    "        os.makedirs(result_dir, exist_ok=True)\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(result_dir, \"model_weights.pth\"))\n",
    "        with open(os.path.join(result_dir, \"logs.json\"), 'w') as f:\n",
    "            json.dump(logs, f, indent=4)\n",
    "        with open(os.path.join(result_dir, \"params.json\"), 'w') as f:\n",
    "            json.dump(params, f, indent=4)\n",
    "        \n",
    "        print(f\"Finished training: {params}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "hyperparams = {\n",
    "    'model_depth': [18, 34],\n",
    "    'batch_size': [16, 32],\n",
    "    'learning_rate': [0.001, 0.0001],\n",
    "    'weight_decay': [0.0001, 0.001],\n",
    "    'dropout': [0.2, 0.5],\n",
    "    'gamma': [True, False],\n",
    "    'gamma_rate': [0.95, 0.99],\n",
    "    'num_epochs': [10, 20],\n",
    "    'optimizer': ['Adam', 'SGD', 'RMSprop']\n",
    "}\n",
    "\n",
    "output_directory = \"./grid_search_results\"\n",
    "\n",
    "datasets = (train_dataset, val_dataset, test_dataset)\n",
    "\n",
    "hyperparameter_search(output_directory, hyperparams, *datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "train_dataset_path = 'H:/Datasets/int_split/Training/'\n",
    "val_dataset_path = 'H:/Datasets/int_split/Validation/'\n",
    "test_dataset_path = 'H:/Datasets/int_split/Testing/'\n",
    "\n",
    "train_dataset = PTDataset(root_dir=train_dataset_path, target_size=(500, 500))\n",
    "val_dataset = PTDataset(root_dir=val_dataset_path, target_size=(500, 500))\n",
    "test_dataset = PTDataset(root_dir=test_dataset_path, target_size=(500, 500))\n",
    "\n",
    "def get_resnet_model(model_depth=34, dropout=0.5, num_classes=4, input_channels=7):\n",
    "    \"\"\"Returns a ResNet model with specified depth and dropout.\"\"\"\n",
    "    if model_depth == 18:\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    elif model_depth == 34:\n",
    "        model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "    elif model_depth == 50:\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid ResNet depth. Choose from 18, 34, or 50.\")\n",
    "    \n",
    "    model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Modify for 7 channels\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    \"\"\"Evaluates the model on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    avg_loss = running_loss / total\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna optimization function with multiple runs per trial.\"\"\"\n",
    "    params = {\n",
    "        'model_depth': trial.suggest_categorical('model_depth', [18, 34]),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-2),\n",
    "        'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-2),\n",
    "        'dropout': trial.suggest_uniform('dropout', 0, 0.9),\n",
    "        'gamma': trial.suggest_categorical('gamma', [True, False]),\n",
    "        'gamma_rate': trial.suggest_uniform('gamma_rate', 0.9, 0.99),\n",
    "        'optimizer': trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'ASGD', 'LBFGS'])\n",
    "    }\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Run each trial 5 times\n",
    "    for run in range(5):\n",
    "        model = get_resnet_model(params['model_depth'], params['dropout']).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if params['optimizer'] == 'Adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'SGD':\n",
    "            optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'], momentum=0.9)\n",
    "        elif params['optimizer'] == 'ASGD':\n",
    "            optimizer = optim.ASGD(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'LBFGS':\n",
    "            optimizer = optim.LBFGS(model.parameters(), lr=params['learning_rate'])  # Removed weight_decay\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer type\")\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=params['gamma_rate']) if params['gamma'] else None\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        for epoch in range(25):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "                \n",
    "                if params['optimizer'] == 'LBFGS':\n",
    "                    optimizer.step(closure)  # Requires closure function\n",
    "                else:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "    # Calculate statistics\n",
    "    min_acc = np.min(val_accuracies)\n",
    "    max_acc = np.max(val_accuracies)\n",
    "    avg_acc = np.mean(val_accuracies)\n",
    "    std_acc = np.std(val_accuracies)\n",
    "    \n",
    "    # Report all statistics\n",
    "    trial.set_user_attr(\"min_accuracy\", min_acc)\n",
    "    trial.set_user_attr(\"max_accuracy\", max_acc)\n",
    "    trial.set_user_attr(\"avg_accuracy\", avg_acc)\n",
    "    trial.set_user_attr(\"std_accuracy\", std_acc)\n",
    "    \n",
    "    # Print statistics for each trial\n",
    "    print(f\"\\nTrial {trial.number} - Results:\")\n",
    "    print(f\"  Parameters: {params}\")\n",
    "    print(f\"  Min Accuracy: {min_acc:.2f}%\")\n",
    "    print(f\"  Max Accuracy: {max_acc:.2f}%\")\n",
    "    print(f\"  Avg Accuracy: {avg_acc:.2f}%\")\n",
    "    print(f\"  Std Dev Accuracy: {std_acc:.2f}%\\n\")\n",
    "    \n",
    "    # Optuna will optimize for average accuracy\n",
    "    return -avg_acc  # Minimizing negative accuracy to maximize positive accuracy\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Display results\n",
    "best_trial = study.best_trial\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Value: {-best_trial.value}\")  # Converting back to positive accuracy\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Display additional statistics\n",
    "print(\"\\n  Additional statistics: \")\n",
    "print(f\"    Min Accuracy: {best_trial.user_attrs['min_accuracy']:.2f}%\")\n",
    "\n",
    "print(f\"    Max Accuracy: {best_trial.user_attrs['max_accuracy']:.2f}%\")\n",
    "print(f\"    Avg Accuracy: {best_trial.user_attrs['avg_accuracy']:.2f}%\")\n",
    "print(f\"    Std Accuracy: {best_trial.user_attrs['std_accuracy']:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
