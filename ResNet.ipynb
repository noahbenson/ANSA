{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Naming Convention\n",
    "\n",
    "Each `.pt` file should follow this format:\n",
    "\n",
    "```\n",
    "<label>_<replicate>.pt\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "- `150_1.pt` → Label: 150 (undetectable), Replicate: 1\n",
    "- `500_2.pt` → Label: 500 (low), Replicate: 2\n",
    "- `7000_3.pt` → Label: 7000 (medium), Replicate: 3\n",
    "- `20000_5.pt` → Label: 20000 (high), Replicate: 5\n",
    "\n",
    "\n",
    "\n",
    "## Class Definitions for Semi-Quantitative approach \n",
    "\n",
    "Infers clinical decision-making based on viral load counts (assuming 1:1 sample prep\n",
    ")\n",
    "1. `undetectable` → Label values `< 200`\n",
    "2. `low` → Label values `200 ≤ label ≤ 1000`\n",
    "3. `medium` → Label values `1000 < label ≤ 10000`\n",
    "4. `high` → Label values `> 10000`\n",
    "\n",
    "# Dataset Folder Structure\n",
    "\n",
    "Dataset is organized into the following structure to ensure proper training, validation, and testing:\n",
    "\n",
    "```\n",
    "Datasets/\n",
    "│-- SemiQuant/\n",
    "│   │-- Training/             # Training dataset (60% of total data)\n",
    "│   │   ├── undetectable/      # Class 0 (e.g., files with labels < 200)\n",
    "│   │   │   ├── 20_1.pt\n",
    "│   │   │   ├── 40_3.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── low/               # Class 1 (200 ≤ label ≤ 1000)\n",
    "|   |   |   ├── 300_2.pt\n",
    "│   │   │   ├── 600_4.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── medium/            # Class 2 (1000 < label ≤ 10000)\n",
    "│   │   │   ├── 2000_1.pt\n",
    "│   │   │   ├── 7000_2.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── high/              # Class 3 (label > 10000)\n",
    "│   │   │   ├── 10000_2.pt\n",
    "│   │   │   ├── 90000_2.pt\n",
    "│   │   │   └── ...\n",
    "│\n",
    "│   │-- Validation/            # Validation dataset (20% of total data)\n",
    "│   │   ├── undetectable/\n",
    "│   │   │   ├── 20_2.pt\n",
    "│   │   │   ├── 40_4.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── low/\n",
    "│   │   ├── medium/\n",
    "│   │   ├── high/\n",
    "│\n",
    "│   │-- Testing/               # Testing dataset (20% of total data)\n",
    "│   │   ├── undetectable/\n",
    "│   │   │   ├── 30_1.pt\n",
    "│   │   │   ├── 50_2.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── low/\n",
    "│   │   ├── medium/\n",
    "│   │   ├── high/\n",
    "│\n",
    "│-- torch_tensors/              # Original .pt files before splitting\n",
    "│   │   ├── 100_1.pt\n",
    "│   │   ├── 200_3.pt\n",
    "│   │   ├── 5000_2.pt\n",
    "│   │   ├── 15000_4.pt\n",
    "│   │   └── ...\n",
    "```\n",
    "\n",
    "## Folder Descriptions\n",
    "\n",
    "- **`Training/`** – Used to train the model (60% of total data).\n",
    "- **`Validation/`** – Used to validate the model during training (20% of total data).\n",
    "- **`Testing/`** – Used to evaluate the model after training (20% of total data).\n",
    "- **`torch_tensors/`** – Stores the original `.pt` files before they were split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTDataset(Dataset):\n",
    "    def __init__(self, root_dir, target_size=(500, 500), transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the dataset directory (e.g., Training folder).\n",
    "            target_size (tuple): Desired output size (height, width).\n",
    "            transform (callable, optional): Optional transformations (on CPU).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "        self.classes = ['undetectable', 'low', 'medium', 'high']\n",
    "\n",
    "        # Collect all file paths and labels\n",
    "        self.file_list = []\n",
    "        for label in self.classes:\n",
    "            class_path = os.path.join(root_dir, label)\n",
    "            if not os.path.exists(class_path):\n",
    "                continue  # Skip if folder doesn't exist\n",
    "            for file in os.listdir(class_path):\n",
    "                if file.endswith('.pt'):\n",
    "                    full_path = os.path.join(class_path, file)\n",
    "                    class_index = self.classes.index(label)\n",
    "                    self.file_list.append((full_path, class_index))\n",
    "\n",
    "        # Pre-load everything into memory (CPU)\n",
    "        self.data_list = []\n",
    "        for file_path, label in self.file_list:\n",
    "            # Load from disk to CPU memory\n",
    "            tensor_data = torch.load(file_path, map_location='cpu')  # [C, T, H, W]\n",
    "\n",
    "            # Ensure enough frames\n",
    "            max_frames = tensor_data.shape[1]\n",
    "            selected_frame_indices = [ 49, 59, 69, 79, 89, 99, 109, 119, 129, 139, 149, 159, 169, 179]\n",
    "            # selected_frame_indices = [69, 89, 109, 129, 149, 179]\n",
    "            selected_frame_indices = [i for i in selected_frame_indices if i < max_frames]\n",
    "            if len(selected_frame_indices) < 6:\n",
    "                raise ValueError(f\"Not enough frames in {file_path}, available: {max_frames}, required: 180\")\n",
    "\n",
    "            # Compute average of the first 20 frames\n",
    "            avg_first_20 = torch.mean(tensor_data[:, :20, :, :], dim=1, keepdim=True)  # [C, 1, H, W]\n",
    "            selected_frames = tensor_data[:, selected_frame_indices, :, :]             # [C, 6, H, W]\n",
    "\n",
    "            # Concatenate to form a 7-frame tensor\n",
    "            final_tensor = torch.cat((avg_first_20, selected_frames), dim=1)  # [C, 7, H, W]\n",
    "            final_tensor = final_tensor.squeeze(0) if final_tensor.shape[0] == 1 else final_tensor\n",
    "\n",
    "            # Resize on CPU\n",
    "            if final_tensor.dim() == 3:\n",
    "                # shape [7, H, W]\n",
    "                final_tensor = final_tensor.unsqueeze(0)  # -> [1, 7, H, W]\n",
    "\n",
    "            resized_tensor = F.interpolate(\n",
    "                final_tensor,\n",
    "                size=self.target_size,\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            # Optional transform\n",
    "            if self.transform:\n",
    "                resized_tensor = self.transform(resized_tensor)\n",
    "\n",
    "            # Model expects input_channels=7, flatten [C=1, frames=7, H, W] -> [7, H, W]\n",
    "            if resized_tensor.shape[0] == 1:\n",
    "                resized_tensor = resized_tensor.squeeze(0)  # shape [7, H, W]\n",
    "\n",
    "            # Store (tensor, label)\n",
    "            self.data_list.append((resized_tensor, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_model(num_classes=4, input_channels=15):\n",
    "    \"\"\"\n",
    "    Build ResNet34 with a custom first conv layer\n",
    "    that expects `input_channels`.\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    # Replace first conv to match your input_channels\n",
    "    model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    # Replace FC layer to match num_classes\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Basic training routine using CrossEntropyLoss\n",
    "    for single-label, multi-class classification.\n",
    "    \"\"\"\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = 100.0 * correct / total\n",
    "\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def plot_confusion_matrix(model, loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Generates and displays a confusion matrix for the model on the given loader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Colem\\AppData\\Local\\Temp\\ipykernel_16888\\3868893834.py:68: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/13] Train Loss: 1.3493, Train Acc: 41.84%, Val Loss: 65.4427, Val Acc: 28.57%\n",
      "Epoch [2/13] Train Loss: nan, Train Acc: 72.34%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [3/13] Train Loss: 0.5133, Train Acc: 82.98%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [4/13] Train Loss: 0.5780, Train Acc: 81.56%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [5/13] Train Loss: nan, Train Acc: 70.92%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [6/13] Train Loss: nan, Train Acc: 61.70%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [7/13] Train Loss: 0.4134, Train Acc: 80.14%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [8/13] Train Loss: 0.2393, Train Acc: 91.49%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [9/13] Train Loss: nan, Train Acc: 60.99%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [10/13] Train Loss: nan, Train Acc: 85.82%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [11/13] Train Loss: nan, Train Acc: 76.60%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [12/13] Train Loss: nan, Train Acc: 75.18%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [13/13] Train Loss: nan, Train Acc: 82.27%, Val Loss: nan, Val Acc: 16.33%\n",
      "Training complete.\n",
      "Test Loss: nan, Test Accuracy: 17.39%\n",
      "Model saved as resnet_model.pth\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1. Check device for cuda or cpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 2. Define dataset paths\n",
    "    train_dataset_path = 'H:/Datasets/int_split/Training/'\n",
    "    val_dataset_path = 'H:/Datasets/int_split/Validation/'\n",
    "    test_dataset_path = 'H:/Datasets/int_split/Testing/'\n",
    "\n",
    "    # 3. Load datasets\n",
    "    train_dataset = PTDataset(root_dir=train_dataset_path, target_size=(500, 500))\n",
    "    val_dataset = PTDataset(root_dir=val_dataset_path, target_size=(500, 500))\n",
    "    test_dataset = PTDataset(root_dir=test_dataset_path, target_size=(500, 500))\n",
    "\n",
    "    # 4. Create WeightedRandomSampler for training\n",
    "    train_labels = [label for _, label in train_dataset.data_list]\n",
    "    class_counts = Counter(train_labels)\n",
    "    weights = [1.0 / class_counts[label] for label in train_labels]\n",
    "\n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=weights,\n",
    "        num_samples=len(weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    # 5. Create DataLoaders\n",
    "    use_pin_memory = True if device.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        sampler=train_sampler,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    # 6. Build model, move to device\n",
    "    model = get_resnet_model(num_classes=4, input_channels=15)\n",
    "    model.to(device)\n",
    "\n",
    "    # 7. Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "\n",
    "    # 8. Train\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=13)\n",
    "\n",
    "    # 9. Evaluate on test set\n",
    "    test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    # 10. Save model\n",
    "    torch.save(model.state_dict(), 'resnet_model_18_10_true_15.pth')\n",
    "    print(\"Model saved as resnet_model.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "#     # If you want to plot the confusion matrix after everything is done:\n",
    "#     # (Just call the function from anywhere outside `main()`.)\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     class_names = ['Und.', 'Low', 'Med', 'High']\n",
    "\n",
    "#     # Re-initialize your model, load state dict if needed, etc.\n",
    "#     model = get_resnet_model(num_classes=4, input_channels=7)\n",
    "#     model.load_state_dict(torch.load('resnet_model_gamma2.pth', map_location=device))\n",
    "#     model.to(device)\n",
    "\n",
    "    # # Re-create test_loader (or pass it around as a global var) # ran out of ram for this to be in the funct\n",
    "    # test_dataset_path = 'M:/Datasets/int_split/Testing/'\n",
    "    # test_dataset = PTDataset(root_dir=test_dataset_path, target_size=(500, 500))\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    # plot_confusion_matrix(model, test_loader, device, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, ConfusionMatrixDisplay\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Ensure the model is in evaluation mode\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Move model to appropriate device (CPU/GPU)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to appropriate device (CPU/GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Collect all true labels and predictions\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:  \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy()) \n",
    "        all_labels.extend(labels.cpu().numpy())  \n",
    "\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "class_names = ['Und.', 'Low', 'Med', 'High']  \n",
    "\n",
    "\n",
    "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of epoch, gamma, and Resnet complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "def get_resnet_model(model_depth=34, num_classes=4, input_channels=15):\n",
    "    \"\"\"Returns a ResNet model with the specified depth (18, 34, 50).\"\"\"\n",
    "    if model_depth == 18:\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    elif model_depth == 34:\n",
    "        model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "    elif model_depth == 50:\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid ResNet depth. Choose from 18, 34, or 50.\")\n",
    "\n",
    "    # Modify first convolution layer to handle 7 input channels\n",
    "    model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_experiment(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    "    model_depth=34,\n",
    "    use_gamma_scheduler=True,\n",
    "    num_epochs=10,\n",
    "    batch_size=32,\n",
    "    experiment_name=\"resnet_experiments.csv\"\n",
    "):\n",
    "    \"\"\"Runs an experiment and logs per-epoch loss & accuracy to CSV.\"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Weighted Sampler for class balancing\n",
    "    train_labels = [label for _, label in train_dataset.data_list]\n",
    "    class_counts = Counter(train_labels)\n",
    "    weights = [1.0 / class_counts[label] for label in train_labels]\n",
    "    train_sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # Initialize Model\n",
    "    model = get_resnet_model(model_depth=model_depth).to(device)\n",
    "\n",
    "    # Define Loss Function, Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Optionally apply Gamma Scheduler\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95) if use_gamma_scheduler else None\n",
    "\n",
    "    # CSV Header (if file doesn't exist)\n",
    "    file_exists = os.path.isfile(experiment_name)\n",
    "    with open(experiment_name, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\n",
    "                \"Epoch\", \"ResNet Depth\", \"Gamma Scheduler\", \"Batch Size\", \"Train Loss\", \n",
    "                \"Train Accuracy\", \"Val Loss\", \"Val Accuracy\", \"Model Parameters\"\n",
    "            ])\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Log epoch data to CSV\n",
    "        with open(experiment_name, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\n",
    "                epoch + 1, model_depth, use_gamma_scheduler, batch_size, \n",
    "                train_loss, train_acc, val_loss, val_acc, \n",
    "                sum(p.numel() for p in model.parameters())\n",
    "            ])\n",
    "\n",
    "    print(f\"Experiment completed and results saved to {experiment_name}\")\n",
    "    return model\n",
    "\n",
    "# Function to Evaluate Model\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.8138, Train Acc: 66.67%, Val Loss: 13.0109, Val Acc: 40.82%\n",
      "Epoch 2: Train Loss: 0.8864, Train Acc: 81.56%, Val Loss: 11.9270, Val Acc: 28.57%\n",
      "Epoch 3: Train Loss: 0.4786, Train Acc: 82.98%, Val Loss: 4.5633, Val Acc: 48.98%\n",
      "Epoch 4: Train Loss: 0.4273, Train Acc: 84.40%, Val Loss: 3.7922, Val Acc: 61.22%\n",
      "Epoch 5: Train Loss: 0.2020, Train Acc: 95.04%, Val Loss: 1.6051, Val Acc: 73.47%\n",
      "Epoch 6: Train Loss: 0.2837, Train Acc: 91.49%, Val Loss: 0.9432, Val Acc: 67.35%\n",
      "Epoch 7: Train Loss: 0.1395, Train Acc: 95.04%, Val Loss: 1.4652, Val Acc: 46.94%\n",
      "Epoch 8: Train Loss: 0.1921, Train Acc: 94.33%, Val Loss: 0.7314, Val Acc: 71.43%\n",
      "Epoch 9: Train Loss: 0.1287, Train Acc: 95.74%, Val Loss: 0.5030, Val Acc: 89.80%\n",
      "Epoch 10: Train Loss: 0.0314, Train Acc: 100.00%, Val Loss: 0.6346, Val Acc: 85.71%\n",
      "Experiment completed and results saved to resnet_experiments.csv\n",
      "Epoch 1: Train Loss: 1.2613, Train Acc: 49.65%, Val Loss: 78.5285, Val Acc: 34.69%\n",
      "Epoch 2: Train Loss: 0.6463, Train Acc: 78.01%, Val Loss: 52.5326, Val Acc: 24.49%\n",
      "Epoch 3: Train Loss: 0.6445, Train Acc: 76.60%, Val Loss: 139.3096, Val Acc: 34.69%\n",
      "Epoch 4: Train Loss: 0.6455, Train Acc: 75.89%, Val Loss: 27.1917, Val Acc: 36.73%\n",
      "Epoch 5: Train Loss: 0.5521, Train Acc: 83.69%, Val Loss: 1.7580, Val Acc: 61.22%\n",
      "Epoch 6: Train Loss: 0.4042, Train Acc: 82.27%, Val Loss: 0.7665, Val Acc: 81.63%\n",
      "Epoch 7: Train Loss: 0.6194, Train Acc: 73.05%, Val Loss: 1.0166, Val Acc: 77.55%\n",
      "Epoch 8: Train Loss: 0.3216, Train Acc: 89.36%, Val Loss: 1.4015, Val Acc: 55.10%\n",
      "Epoch 9: Train Loss: 0.3559, Train Acc: 85.11%, Val Loss: 0.5741, Val Acc: 79.59%\n",
      "Epoch 10: Train Loss: 0.3520, Train Acc: 85.82%, Val Loss: 0.7360, Val Acc: 73.47%\n",
      "Experiment completed and results saved to resnet_experiments.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(15, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_path = 'H:/Datasets/int_split/Training/'\n",
    "val_dataset_path = 'H:/Datasets/int_split/Validation/'\n",
    "test_dataset_path = 'H:/Datasets/int_split/Testing/'\n",
    "\n",
    "train_dataset = PTDataset(root_dir=train_dataset_path, target_size=(500, 500))\n",
    "val_dataset = PTDataset(root_dir=val_dataset_path, target_size=(500, 500))\n",
    "test_dataset = PTDataset(root_dir=test_dataset_path, target_size=(500, 500))\n",
    "\n",
    "# # Experiment 1: ResNet-34, No gamma scheduler, 10 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=34,\n",
    "#     use_gamma_scheduler=False,\n",
    "#     num_epochs=10,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 2: ResNet-34, No gamma scheduler, 10 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=34,\n",
    "#     use_gamma_scheduler=False,\n",
    "#     num_epochs=10,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 3: ResNet-34, Gamma scheduler, 10 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=34,\n",
    "#     use_gamma_scheduler= True,\n",
    "#     num_epochs=10,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 4: ResNet-34, Gamma scheduler, 10 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=34,\n",
    "#     use_gamma_scheduler= True,\n",
    "#     num_epochs=10,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 5: ResNet-34, Gamma scheduler, 13 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=34,\n",
    "#     use_gamma_scheduler= True,\n",
    "#     num_epochs=13,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 6: ResNet-34, Gamma scheduler, 13 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=34,\n",
    "#     use_gamma_scheduler= True,\n",
    "#     num_epochs=13,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 7: ResNet-34, Gamma scheduler, 30 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=34,\n",
    "#     use_gamma_scheduler= True,\n",
    "#     num_epochs=30,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 8: ResNet-34, Gamma scheduler, 30 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=34,\n",
    "#     use_gamma_scheduler= True,\n",
    "#     num_epochs=30,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 9: ResNet-18, No amma scheduler, 10 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=18,\n",
    "#     use_gamma_scheduler= False,\n",
    "#     num_epochs=10,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 10: ResNet-18, No amma scheduler, 10 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=18,\n",
    "#     use_gamma_scheduler= False,\n",
    "#     num_epochs=10,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 11: ResNet-50, No gamma scheduler, 10 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=50,\n",
    "#     use_gamma_scheduler=False,\n",
    "#     num_epochs=20,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 12: ResNet-50, No gamma scheduler, 10 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=50,\n",
    "#     use_gamma_scheduler=False,\n",
    "#     num_epochs=20,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# \n",
    "# Experiment 13: ResNet-50, No gamma scheduler, 10 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=50,\n",
    "#     use_gamma_scheduler=False,\n",
    "#     num_epochs=10,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 14: ResNet-50, No gamma scheduler, 10 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=50,\n",
    "#     use_gamma_scheduler=False,\n",
    "#     num_epochs=10,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# Experiment 15: ResNet-18, Gamma scheduler, 13 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=18,\n",
    "#     use_gamma_scheduler=True,\n",
    "#     num_epochs=10,\n",
    "#     batch_size=13,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 16: ResNet-18, Gamma scheduler, 13 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=18,\n",
    "#     use_gamma_scheduler=True,\n",
    "#     num_epochs=13,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 17: ResNet-18, Gamma scheduler, 30 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=18,\n",
    "#     use_gamma_scheduler=True,\n",
    "#     num_epochs=30,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 18: ResNet-18, Gamma scheduler, 30 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=18,\n",
    "#     use_gamma_scheduler=True,\n",
    "#     num_epochs=30,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "#  # Experiment 18: ResNet-18, Gamma scheduler, 30 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=18,\n",
    "#     use_gamma_scheduler=True,\n",
    "#     num_epochs=10,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n",
    "# # Experiment 18: ResNet-18, Gamma scheduler, 30 epochs\n",
    "# run_experiment(\n",
    "#     train_dataset, val_dataset, test_dataset,\n",
    "#     model_depth=34,\n",
    "#     use_gamma_scheduler=True,\n",
    "#     num_epochs=10,\n",
    "#     batch_size=32,\n",
    "#     experiment_name=\"resnet_experiments.csv\"\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "def hyperparameter_search(output_dir, hyperparams, train_dataset, val_dataset, test_dataset):\n",
    "    \"\"\"\n",
    "    Perform a grid search over the given hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): Directory to save results\n",
    "        hyperparams (dict): Dictionary of hyperparameters to search\n",
    "        train_dataset, val_dataset, test_dataset: Datasets for training, validation, and testing\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Get all hyperparameter combinations\n",
    "    keys, values = zip(*hyperparams.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    \n",
    "    for params in param_combinations:\n",
    "        model_depth = params['model_depth']\n",
    "        batch_size = params['batch_size']\n",
    "        learning_rate = params['learning_rate']\n",
    "        weight_decay = params['weight_decay']\n",
    "        dropout = params['dropout']\n",
    "        gamma = params['gamma']\n",
    "        gamma_rate = params['gamma_rate']\n",
    "        num_epochs = params['num_epochs']\n",
    "        optimizer_type = params['optimizer']\n",
    "        \n",
    "        # Weighted sampler for balancing dataset\n",
    "        train_labels = [label for _, label in train_dataset.data_list]\n",
    "        class_counts = Counter(train_labels)\n",
    "        weights = [1.0 / class_counts[label] for label in train_labels]\n",
    "        train_sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "        \n",
    "        # DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = get_resnet_model(model_depth=model_depth, dropout=dropout).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        if optimizer_type == 'Adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'SGD':\n",
    "            optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)\n",
    "        elif optimizer_type == 'RMSprop':\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer type\")\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma_rate) if gamma else None\n",
    "        \n",
    "        logs = []\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss, correct, total = 0.0, 0, 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "            \n",
    "            train_loss = running_loss / total\n",
    "            train_acc = 100.0 * correct / total\n",
    "            val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            logs.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc\n",
    "            })\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Save results\n",
    "        result_dir = os.path.join(output_dir, f\"model_{model_depth}_batch_{batch_size}_lr_{learning_rate}_wd_{weight_decay}_dropout_{dropout}_gamma_{gamma}_gammaRate_{gamma_rate}_epochs_{num_epochs}_opt_{optimizer_type}\")\n",
    "        os.makedirs(result_dir, exist_ok=True)\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(result_dir, \"model_weights.pth\"))\n",
    "        with open(os.path.join(result_dir, \"logs.json\"), 'w') as f:\n",
    "            json.dump(logs, f, indent=4)\n",
    "        with open(os.path.join(result_dir, \"params.json\"), 'w') as f:\n",
    "            json.dump(params, f, indent=4)\n",
    "        \n",
    "        print(f\"Finished training: {params}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "hyperparams = {\n",
    "    'model_depth': [18, 34],\n",
    "    'batch_size': [16, 32],\n",
    "    'learning_rate': [0.001, 0.0001],\n",
    "    'weight_decay': [0.0001, 0.001],\n",
    "    'dropout': [0.2, 0.5],\n",
    "    'gamma': [True, False],\n",
    "    'gamma_rate': [0.95, 0.99],\n",
    "    'num_epochs': [10, 20],\n",
    "    'optimizer': ['Adam', 'SGD', 'RMSprop']\n",
    "}\n",
    "\n",
    "output_directory = \"./grid_search_results\"\n",
    "\n",
    "datasets = (train_dataset, val_dataset, test_dataset)\n",
    "\n",
    "hyperparameter_search(output_directory, hyperparams, *datasets)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANSA_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
