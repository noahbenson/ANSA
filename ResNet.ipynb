{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Naming Convention\n",
    "\n",
    "Each `.pt` file should follow this format:\n",
    "\n",
    "```\n",
    "<label>_<replicate>.pt\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "- `150_1.pt` → Label: 150 (undetectable), Replicate: 1\n",
    "- `500_2.pt` → Label: 500 (low), Replicate: 2\n",
    "- `7000_3.pt` → Label: 7000 (medium), Replicate: 3\n",
    "- `20000_5.pt` → Label: 20000 (high), Replicate: 5\n",
    "\n",
    "\n",
    "\n",
    "## Class Definitions for Semi-Quantitative approach \n",
    "\n",
    "Infers clinical decision-making based on viral load counts (assuming 1:1 sample prep\n",
    ")\n",
    "1. `undetectable` → Label values `< 200`\n",
    "2. `low` → Label values `200 ≤ label ≤ 1000`\n",
    "3. `medium` → Label values `1000 < label ≤ 10000`\n",
    "4. `high` → Label values `> 10000`\n",
    "\n",
    "# Dataset Folder Structure\n",
    "\n",
    "Dataset is organized into the following structure to ensure proper training, validation, and testing:\n",
    "\n",
    "```\n",
    "Datasets/\n",
    "│-- SemiQuant/\n",
    "│   │-- Training/             # Training dataset (60% of total data)\n",
    "│   │   ├── undetectable/      # Class 0 (e.g., files with labels < 200)\n",
    "│   │   │   ├── 20_1.pt\n",
    "│   │   │   ├── 40_3.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── low/               # Class 1 (200 ≤ label ≤ 1000)\n",
    "|   |   |   ├── 300_2.pt\n",
    "│   │   │   ├── 600_4.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── medium/            # Class 2 (1000 < label ≤ 10000)\n",
    "│   │   │   ├── 2000_1.pt\n",
    "│   │   │   ├── 7000_2.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── high/              # Class 3 (label > 10000)\n",
    "│   │   │   ├── 10000_2.pt\n",
    "│   │   │   ├── 90000_2.pt\n",
    "│   │   │   └── ...\n",
    "│\n",
    "│   │-- Validation/            # Validation dataset (20% of total data)\n",
    "│   │   ├── undetectable/\n",
    "│   │   │   ├── 20_2.pt\n",
    "│   │   │   ├── 40_4.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── low/\n",
    "│   │   ├── medium/\n",
    "│   │   ├── high/\n",
    "│\n",
    "│   │-- Testing/               # Testing dataset (20% of total data)\n",
    "│   │   ├── undetectable/\n",
    "│   │   │   ├── 30_1.pt\n",
    "│   │   │   ├── 50_2.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── low/\n",
    "│   │   ├── medium/\n",
    "│   │   ├── high/\n",
    "│\n",
    "│-- torch_tensors/              # Original .pt files before splitting\n",
    "│   │   ├── 100_1.pt\n",
    "│   │   ├── 200_3.pt\n",
    "│   │   ├── 5000_2.pt\n",
    "│   │   ├── 15000_4.pt\n",
    "│   │   └── ...\n",
    "```\n",
    "\n",
    "## Folder Descriptions\n",
    "\n",
    "- **`Training/`** – Used to train the model (60% of total data).\n",
    "- **`Validation/`** – Used to validate the model during training (20% of total data).\n",
    "- **`Testing/`** – Used to evaluate the model after training (20% of total data).\n",
    "- **`torch_tensors/`** – Stores the original `.pt` files before they were split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTDataset(Dataset):\n",
    "    def __init__(self, root_dir, target_size=(500, 500), transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the dataset directory (e.g., Training folder).\n",
    "            target_size (tuple): Desired output size (height, width).\n",
    "            transform (callable, optional): Optional transformations (on CPU).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "        self.classes = ['undetectable', 'low', 'medium', 'high']\n",
    "        # self.classes = ['0.undetectable', '1.low', '2.medium', '3.high', '4.very high']\n",
    "\n",
    "\n",
    "        # Collect all file paths and labels\n",
    "        self.file_list = []\n",
    "        for label in self.classes:\n",
    "            class_path = os.path.join(root_dir, label)\n",
    "            if not os.path.exists(class_path):\n",
    "                continue  # Skip if folder doesn't exist\n",
    "            for file in os.listdir(class_path):\n",
    "                if file.endswith('.pt'):\n",
    "                    full_path = os.path.join(class_path, file)\n",
    "                    class_index = self.classes.index(label)\n",
    "                    self.file_list.append((full_path, class_index))\n",
    "\n",
    "        # Pre-load everything into memory (CPU)\n",
    "        self.data_list = []\n",
    "        for file_path, label in self.file_list:\n",
    "            # Load from disk to CPU memory\n",
    "            tensor_data = torch.load(file_path, map_location='cpu')  # [C, T, H, W]\n",
    "\n",
    "            # Ensure enough frames\n",
    "            max_frames = tensor_data.shape[1]\n",
    "            # selected_frame_indices = [ 49, 59, 69, 79, 89, 99, 109, 119, 129, 139, 149, 159, 169, 179]\n",
    "            selected_frame_indices = [69, 89, 109, 129, 149, 179]\n",
    "            selected_frame_indices = [i for i in selected_frame_indices if i < max_frames]\n",
    "            if len(selected_frame_indices) < 6:\n",
    "                raise ValueError(f\"Not enough frames in {file_path}, available: {max_frames}, required: 180\")\n",
    "\n",
    "            # Compute average of the first 20 frames\n",
    "            avg_first_20 = torch.mean(tensor_data[:, :20, :, :], dim=1, keepdim=True)  # [C, 1, H, W]\n",
    "            selected_frames = tensor_data[:, selected_frame_indices, :, :]             # [C, 6, H, W]\n",
    "\n",
    "            # Concatenate to form a 7-frame tensor\n",
    "            final_tensor = torch.cat((avg_first_20, selected_frames), dim=1)  # [C, 7, H, W]\n",
    "            final_tensor = final_tensor.squeeze(0) if final_tensor.shape[0] == 1 else final_tensor\n",
    "\n",
    "            # Resize on CPU\n",
    "            if final_tensor.dim() == 3:\n",
    "                # shape [7, H, W]\n",
    "                final_tensor = final_tensor.unsqueeze(0)  # -> [1, 7, H, W]\n",
    "\n",
    "            resized_tensor = F.interpolate(\n",
    "                final_tensor,\n",
    "                size=self.target_size,\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            # Optional transform\n",
    "            if self.transform:\n",
    "                resized_tensor = self.transform(resized_tensor)\n",
    "\n",
    "            # Model expects input_channels=7, flatten [C=1, frames=7, H, W] -> [7, H, W]\n",
    "            if resized_tensor.shape[0] == 1:\n",
    "                resized_tensor = resized_tensor.squeeze(0)  # shape [7, H, W]\n",
    "\n",
    "            # Store (tensor, label)\n",
    "            self.data_list.append((resized_tensor, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Labels in dataset: {0, 1, 2, 3}\n",
      "\n",
      "=== Training Run 1 of 5 ===\n",
      "Epoch [1/18] Train Loss: 1.3560, Train Acc: 41.84%, Val Loss: 1.2972, Val Acc: 59.18%\n",
      "Epoch [2/18] Train Loss: 0.8087, Train Acc: 70.21%, Val Loss: 0.7574, Val Acc: 73.47%\n",
      "Epoch [3/18] Train Loss: 0.5634, Train Acc: 75.89%, Val Loss: 0.6552, Val Acc: 83.67%\n",
      "Epoch [4/18] Train Loss: 0.2702, Train Acc: 89.36%, Val Loss: 0.5988, Val Acc: 85.71%\n",
      "Epoch [5/18] Train Loss: 0.3153, Train Acc: 87.94%, Val Loss: 0.7486, Val Acc: 79.59%\n",
      "Epoch [6/18] Train Loss: nan, Train Acc: 60.28%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [7/18] Train Loss: 0.1849, Train Acc: 94.33%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [8/18] Train Loss: 0.2362, Train Acc: 90.78%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [9/18] Train Loss: 0.0870, Train Acc: 96.45%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [10/18] Train Loss: nan, Train Acc: 78.01%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [11/18] Train Loss: 0.1354, Train Acc: 95.74%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [12/18] Train Loss: nan, Train Acc: 57.45%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [13/18] Train Loss: 0.0933, Train Acc: 97.87%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [14/18] Train Loss: nan, Train Acc: 83.69%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [15/18] Train Loss: 0.0748, Train Acc: 97.16%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [16/18] Train Loss: 0.0598, Train Acc: 99.29%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [17/18] Train Loss: nan, Train Acc: 84.40%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [18/18] Train Loss: 0.1037, Train Acc: 97.16%, Val Loss: nan, Val Acc: 16.33%\n",
      "Training complete.\n",
      "Run 1 validation accuracy: 16.33%\n",
      "New best model found with val_acc=16.33% (Run 1).\n",
      "\n",
      "=== Training Run 2 of 5 ===\n",
      "Epoch [1/18] Train Loss: 1.3051, Train Acc: 44.68%, Val Loss: 1.2844, Val Acc: 30.61%\n",
      "Epoch [2/18] Train Loss: 0.6459, Train Acc: 76.60%, Val Loss: 0.9069, Val Acc: 63.27%\n",
      "Epoch [3/18] Train Loss: 0.5299, Train Acc: 76.60%, Val Loss: 0.5019, Val Acc: 87.76%\n",
      "Epoch [4/18] Train Loss: 0.1889, Train Acc: 96.45%, Val Loss: 0.4486, Val Acc: 91.84%\n",
      "Epoch [5/18] Train Loss: nan, Train Acc: 78.72%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [6/18] Train Loss: 0.1529, Train Acc: 94.33%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [7/18] Train Loss: 0.2089, Train Acc: 90.78%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [8/18] Train Loss: nan, Train Acc: 80.85%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [9/18] Train Loss: 0.2554, Train Acc: 90.78%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [10/18] Train Loss: nan, Train Acc: 84.40%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [11/18] Train Loss: nan, Train Acc: 80.14%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [12/18] Train Loss: 0.0906, Train Acc: 96.45%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [13/18] Train Loss: 0.0851, Train Acc: 98.58%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [14/18] Train Loss: nan, Train Acc: 64.54%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [15/18] Train Loss: 0.1223, Train Acc: 94.33%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [16/18] Train Loss: 0.0481, Train Acc: 97.87%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [17/18] Train Loss: nan, Train Acc: 69.50%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [18/18] Train Loss: 0.0181, Train Acc: 100.00%, Val Loss: nan, Val Acc: 16.33%\n",
      "Training complete.\n",
      "Run 2 validation accuracy: 16.33%\n",
      "\n",
      "=== Training Run 3 of 5 ===\n",
      "Epoch [1/18] Train Loss: 1.3312, Train Acc: 35.46%, Val Loss: 1.1575, Val Acc: 53.06%\n",
      "Epoch [2/18] Train Loss: 0.7478, Train Acc: 78.01%, Val Loss: 0.7541, Val Acc: 69.39%\n",
      "Epoch [3/18] Train Loss: 0.3388, Train Acc: 90.07%, Val Loss: 0.5932, Val Acc: 83.67%\n",
      "Epoch [4/18] Train Loss: 0.2676, Train Acc: 92.20%, Val Loss: 0.6025, Val Acc: 83.67%\n",
      "Epoch [5/18] Train Loss: 0.2310, Train Acc: 90.07%, Val Loss: 0.6148, Val Acc: 89.80%\n",
      "Epoch [6/18] Train Loss: 0.1451, Train Acc: 92.20%, Val Loss: 0.5841, Val Acc: 83.67%\n",
      "Epoch [7/18] Train Loss: nan, Train Acc: 77.30%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [8/18] Train Loss: nan, Train Acc: 80.14%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [9/18] Train Loss: nan, Train Acc: 80.14%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [10/18] Train Loss: 0.1274, Train Acc: 95.74%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [11/18] Train Loss: nan, Train Acc: 60.28%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [12/18] Train Loss: nan, Train Acc: 85.82%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [13/18] Train Loss: 0.0973, Train Acc: 95.74%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [14/18] Train Loss: nan, Train Acc: 78.72%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [15/18] Train Loss: 0.1176, Train Acc: 97.16%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [16/18] Train Loss: nan, Train Acc: 90.78%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [17/18] Train Loss: nan, Train Acc: 79.43%, Val Loss: nan, Val Acc: 16.33%\n",
      "Epoch [18/18] Train Loss: 0.0720, Train Acc: 97.87%, Val Loss: nan, Val Acc: 16.33%\n",
      "Training complete.\n",
      "Run 3 validation accuracy: 16.33%\n",
      "\n",
      "=== Training Run 4 of 5 ===\n",
      "Epoch [1/18] Train Loss: 1.2180, Train Acc: 55.32%, Val Loss: 1.1021, Val Acc: 48.98%\n",
      "Epoch [2/18] Train Loss: 0.8176, Train Acc: 68.79%, Val Loss: 0.7203, Val Acc: 81.63%\n",
      "Epoch [3/18] Train Loss: 0.5017, Train Acc: 81.56%, Val Loss: 0.6008, Val Acc: 83.67%\n",
      "Epoch [4/18] Train Loss: 0.3997, Train Acc: 84.40%, Val Loss: 0.6996, Val Acc: 85.71%\n",
      "Epoch [5/18] Train Loss: 0.4087, Train Acc: 82.98%, Val Loss: 0.5390, Val Acc: 85.71%\n",
      "Epoch [6/18] Train Loss: 0.1942, Train Acc: 95.74%, Val Loss: 0.7873, Val Acc: 73.47%\n",
      "Epoch [7/18] Train Loss: 0.1989, Train Acc: 92.20%, Val Loss: 0.5676, Val Acc: 87.76%\n",
      "Epoch [8/18] Train Loss: 0.1575, Train Acc: 95.04%, Val Loss: 0.6742, Val Acc: 85.71%\n",
      "Epoch [9/18] Train Loss: 0.1609, Train Acc: 93.62%, Val Loss: 0.7555, Val Acc: 87.76%\n",
      "Epoch [10/18] Train Loss: 0.0885, Train Acc: 97.16%, Val Loss: 1.0214, Val Acc: 77.55%\n",
      "Epoch [11/18] Train Loss: 0.1631, Train Acc: 94.33%, Val Loss: 0.7438, Val Acc: 83.67%\n",
      "Epoch [12/18] Train Loss: 0.1856, Train Acc: 90.78%, Val Loss: 0.6310, Val Acc: 85.71%\n",
      "Epoch [13/18] Train Loss: 0.2026, Train Acc: 90.07%, Val Loss: 0.5987, Val Acc: 83.67%\n",
      "Epoch [14/18] Train Loss: 0.1125, Train Acc: 96.45%, Val Loss: 0.8435, Val Acc: 77.55%\n",
      "Epoch [15/18] Train Loss: 0.1199, Train Acc: 94.33%, Val Loss: 0.7971, Val Acc: 79.59%\n",
      "Epoch [16/18] Train Loss: 0.0309, Train Acc: 100.00%, Val Loss: 0.6844, Val Acc: 85.71%\n",
      "Epoch [17/18] Train Loss: 0.1057, Train Acc: 97.16%, Val Loss: 0.6138, Val Acc: 81.63%\n",
      "Epoch [18/18] Train Loss: 0.0773, Train Acc: 97.87%, Val Loss: 0.6322, Val Acc: 83.67%\n",
      "Training complete.\n",
      "Run 4 validation accuracy: 83.67%\n",
      "New best model found with val_acc=83.67% (Run 4).\n",
      "\n",
      "=== Training Run 5 of 5 ===\n",
      "Epoch [1/18] Train Loss: 1.2768, Train Acc: 39.01%, Val Loss: 1.2439, Val Acc: 40.82%\n",
      "Epoch [2/18] Train Loss: 0.6900, Train Acc: 78.01%, Val Loss: 0.8210, Val Acc: 65.31%\n",
      "Epoch [3/18] Train Loss: 0.5491, Train Acc: 81.56%, Val Loss: 0.5736, Val Acc: 89.80%\n",
      "Epoch [4/18] Train Loss: 0.4819, Train Acc: 80.14%, Val Loss: 0.8720, Val Acc: 69.39%\n",
      "Epoch [5/18] Train Loss: 0.3084, Train Acc: 87.94%, Val Loss: 0.6630, Val Acc: 85.71%\n",
      "Epoch [6/18] Train Loss: 0.1988, Train Acc: 92.91%, Val Loss: 0.7775, Val Acc: 83.67%\n",
      "Epoch [7/18] Train Loss: 0.2687, Train Acc: 92.20%, Val Loss: 0.6191, Val Acc: 87.76%\n",
      "Epoch [8/18] Train Loss: 0.1767, Train Acc: 94.33%, Val Loss: 0.6041, Val Acc: 85.71%\n",
      "Epoch [9/18] Train Loss: 0.1133, Train Acc: 96.45%, Val Loss: 0.7384, Val Acc: 77.55%\n",
      "Epoch [10/18] Train Loss: 0.1320, Train Acc: 95.74%, Val Loss: 0.6003, Val Acc: 85.71%\n",
      "Epoch [11/18] Train Loss: 0.1040, Train Acc: 97.16%, Val Loss: 0.6001, Val Acc: 87.76%\n",
      "Epoch [12/18] Train Loss: 0.1322, Train Acc: 96.45%, Val Loss: 0.6955, Val Acc: 91.84%\n",
      "Epoch [13/18] Train Loss: 0.1450, Train Acc: 94.33%, Val Loss: 0.7365, Val Acc: 83.67%\n",
      "Epoch [14/18] Train Loss: 0.1038, Train Acc: 97.87%, Val Loss: 0.5753, Val Acc: 87.76%\n",
      "Epoch [15/18] Train Loss: 0.0619, Train Acc: 98.58%, Val Loss: 0.5995, Val Acc: 87.76%\n",
      "Epoch [16/18] Train Loss: 0.0875, Train Acc: 97.16%, Val Loss: 0.5583, Val Acc: 85.71%\n",
      "Epoch [17/18] Train Loss: 0.0498, Train Acc: 99.29%, Val Loss: 0.5907, Val Acc: 85.71%\n",
      "Epoch [18/18] Train Loss: 0.1136, Train Acc: 97.16%, Val Loss: 1.0950, Val Acc: 79.59%\n",
      "Training complete.\n",
      "Run 5 validation accuracy: 79.59%\n",
      "\n",
      "Best model saved with val_acc=83.67%\n",
      "Test Loss: 0.4405, Test Accuracy: 82.61% for the best model\n"
     ]
    }
   ],
   "source": [
    "def get_resnet_model(num_classes=4, input_channels=7, dropout_rate=0.243493213909431):\n",
    "    \"\"\"\n",
    "    Build ResNet18 with a custom first conv layer\n",
    "    that expects `input_channels` and adds a Dropout layer.\n",
    "\n",
    "    model_depth = 18 (ResNet18)\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    \n",
    "    # Replace first conv to match your input_channels\n",
    "    model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    \n",
    "    # Replace FC layer to include Dropout before classification\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_rate),  # Dropout before final classification\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            # Use AMP if on GPU\n",
    "            with torch.amp.autocast(device_type='cuda', enabled=(device.type == 'cuda')):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=25):\n",
    "    \"\"\"\n",
    "    Basic training routine using CrossEntropyLoss\n",
    "    for single-label, multi-class classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            with torch.amp.autocast(device_type='cuda', enabled=(device.type == 'cuda')):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = 100.0 * correct / total\n",
    "\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "def plot_confusion_matrix(model, loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Generates and displays a confusion matrix for the model on the given loader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train_dataset_path = 'H:/Datasets/int_split/Training/'\n",
    "val_dataset_path   = 'H:/Datasets/int_split/Validation/'\n",
    "test_dataset_path  = 'H:/Datasets/int_split/Testing/'\n",
    "\n",
    "# train_dataset_path = 'H:/Datasets/Log_split/Training/'\n",
    "# val_dataset_path   = 'H:/Datasets/Log_split/Validation/'\n",
    "# test_dataset_path  = 'H:/Datasets/Log_split/Testing/'\n",
    "\n",
    "def main():\n",
    "    #Check device for CUDA or CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    \n",
    "    train_dataset = PTDataset(root_dir=train_dataset_path, target_size=(500, 500))\n",
    "    val_dataset   = PTDataset(root_dir=val_dataset_path,   target_size=(500, 500))\n",
    "    test_dataset  = PTDataset(root_dir=test_dataset_path,  target_size=(500, 500))\n",
    "\n",
    "   \n",
    "    train_labels = [label for _, label in train_dataset.data_list]\n",
    "    print(\"Labels in dataset:\", set(train_labels))\n",
    "\n",
    "    class_counts = Counter(train_labels)\n",
    "    weights = [1.0 / class_counts[label] for label in train_labels]\n",
    "    \n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=weights,\n",
    "        num_samples=len(weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    use_pin_memory = (device.type == 'cuda')\n",
    "    batch_size = 32 \n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    \n",
    "    best_val_acc = -float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    # Train/Evaluate 5 times\n",
    "    for run_idx in range(5):\n",
    "        print(f\"\\n=== Training Run {run_idx+1} of 5 ===\")\n",
    "\n",
    "        \n",
    "        model = get_resnet_model(num_classes=4, input_channels=7)  \n",
    "        model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # learning_rate = 0.0038825206157311557\n",
    "        # weight_decay  = 0.0001931053552153856\n",
    "        # gamma_rate    = 0.9388047294838997\n",
    "\n",
    "        learning_rate = 0.00408909107539898\n",
    "        weight_decay  = 0.0000217293313972603\n",
    "        gamma_rate    = 0.972442722810877\n",
    "        \n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            momentum=0.9\n",
    "        )\n",
    "\n",
    "        # optimizer = optim.Adam(\n",
    "        #     model.parameters(),\n",
    "        #     lr=learning_rate,\n",
    "        #     weight_decay=weight_decay\n",
    "        # )\n",
    "\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=gamma_rate)\n",
    "\n",
    "        \n",
    "        num_epochs = 18\n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            num_epochs=num_epochs\n",
    "        )\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        print(f\"Run {run_idx+1} validation accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        # Keep track of best model so far\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"New best model found with val_acc={val_acc:.2f}% (Run {run_idx+1}).\")\n",
    "\n",
    "    # After all 5 runs, save only the best model\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, 'resnet_model_3\n",
    "                   .pth')\n",
    "        print(f\"\\nBest model saved with val_acc={best_val_acc:.2f}%\")\n",
    "\n",
    "        best_model = get_resnet_model(num_classes=4, input_channels=7)\n",
    "        best_model.load_state_dict(best_model_state)\n",
    "        best_model.to(device)\n",
    "\n",
    "        test_loss, test_acc = evaluate_model(best_model, test_loader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}% for the best model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def load_best_model(model_path, num_classes=4, input_channels=7, dropout_rate=0.243493213909431):\n",
    "    \"\"\"\n",
    "    Load the best trained model from saved weights.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    model.conv1 = torch.nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.fc = torch.nn.Sequential(\n",
    "        torch.nn.Dropout(p=dropout_rate),\n",
    "        torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()  \n",
    "\n",
    "    return model, device\n",
    "\n",
    "def plot_confusion_matrix(model, loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Generates and displays a confusion matrix for the model on the given loader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "model_path = \"resnet_model_optuna.pth\"\n",
    "model, device = load_best_model(model_path)\n",
    "\n",
    "test_dataset_path = 'H:/Datasets/int_split/Testing/'\n",
    "test_dataset = PTDataset(root_dir=test_dataset_path, target_size=(500, 500))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=(device.type == 'cuda')\n",
    ")\n",
    "\n",
    "class_names = ['Und.', 'Low', 'Med', 'High']  \n",
    "\n",
    "plot_confusion_matrix(model, test_loader, device, class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "def hyperparameter_search(output_dir, hyperparams, train_dataset, val_dataset, test_dataset):\n",
    "    \"\"\"\n",
    "    Perform a grid search over the given hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): Directory to save results\n",
    "        hyperparams (dict): Dictionary of hyperparameters to search\n",
    "        train_dataset, val_dataset, test_dataset: Datasets for training, validation, and testing\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Get all hyperparameter combinations\n",
    "    keys, values = zip(*hyperparams.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    \n",
    "    for params in param_combinations:\n",
    "        model_depth = params['model_depth']\n",
    "        batch_size = params['batch_size']\n",
    "        learning_rate = params['learning_rate']\n",
    "        weight_decay = params['weight_decay']\n",
    "        dropout = params['dropout']\n",
    "        gamma = params['gamma']\n",
    "        gamma_rate = params['gamma_rate']\n",
    "        num_epochs = params['num_epochs']\n",
    "        optimizer_type = params['optimizer']\n",
    "        \n",
    "        # Weighted sampler for balancing dataset\n",
    "        train_labels = [label for _, label in train_dataset.data_list]\n",
    "        class_counts = Counter(train_labels)\n",
    "        weights = [1.0 / class_counts[label] for label in train_labels]\n",
    "        train_sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "        \n",
    "        # DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = get_resnet_model(model_depth=model_depth, dropout=dropout).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        if optimizer_type == 'Adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'SGD':\n",
    "            optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)\n",
    "        elif optimizer_type == 'RMSprop':\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer type\")\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma_rate) if gamma else None\n",
    "        \n",
    "        logs = []\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss, correct, total = 0.0, 0, 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "            \n",
    "            train_loss = running_loss / total\n",
    "            train_acc = 100.0 * correct / total\n",
    "            val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            logs.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc\n",
    "            })\n",
    "        \n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Save results\n",
    "        result_dir = os.path.join(output_dir, f\"model_{model_depth}_batch_{batch_size}_lr_{learning_rate}_wd_{weight_decay}_dropout_{dropout}_gamma_{gamma}_gammaRate_{gamma_rate}_epochs_{num_epochs}_opt_{optimizer_type}\")\n",
    "        os.makedirs(result_dir, exist_ok=True)\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(result_dir, \"model_weights.pth\"))\n",
    "        with open(os.path.join(result_dir, \"logs.json\"), 'w') as f:\n",
    "            json.dump(logs, f, indent=4)\n",
    "        with open(os.path.join(result_dir, \"params.json\"), 'w') as f:\n",
    "            json.dump(params, f, indent=4)\n",
    "        \n",
    "        print(f\"Finished training: {params}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "hyperparams = {\n",
    "    'model_depth': [18, 34],\n",
    "    'batch_size': [16, 32],\n",
    "    'learning_rate': [0.001, 0.0001],\n",
    "    'weight_decay': [0.0001, 0.001],\n",
    "    'dropout': [0.2, 0.5],\n",
    "    'gamma': [True, False],\n",
    "    'gamma_rate': [0.95, 0.99],\n",
    "    'num_epochs': [10, 20],\n",
    "    'optimizer': ['Adam', 'SGD', 'RMSprop']\n",
    "}\n",
    "\n",
    "output_directory = \"./grid_search_results\"\n",
    "\n",
    "datasets = (train_dataset, val_dataset, test_dataset)\n",
    "\n",
    "hyperparameter_search(output_directory, hyperparams, *datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "train_dataset_path = 'H:/Datasets/int_split/Training/'\n",
    "val_dataset_path = 'H:/Datasets/int_split/Validation/'\n",
    "test_dataset_path = 'H:/Datasets/int_split/Testing/'\n",
    "\n",
    "train_dataset = PTDataset(root_dir=train_dataset_path, target_size=(500, 500))\n",
    "val_dataset = PTDataset(root_dir=val_dataset_path, target_size=(500, 500))\n",
    "test_dataset = PTDataset(root_dir=test_dataset_path, target_size=(500, 500))\n",
    "\n",
    "def get_resnet_model(model_depth=34, dropout=0.5, num_classes=4, input_channels=7):\n",
    "    \"\"\"Returns a ResNet model with specified depth and dropout.\"\"\"\n",
    "    if model_depth == 18:\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    elif model_depth == 34:\n",
    "        model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "    elif model_depth == 50:\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid ResNet depth. Choose from 18, 34, or 50.\")\n",
    "    \n",
    "    model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Modify for 7 channels\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    \"\"\"Evaluates the model on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    avg_loss = running_loss / total\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna optimization function with multiple runs per trial.\"\"\"\n",
    "    params = {\n",
    "        'model_depth': trial.suggest_categorical('model_depth', [18, 34]),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-2),\n",
    "        'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-2),\n",
    "        'dropout': trial.suggest_uniform('dropout', 0, 0.9),\n",
    "        'gamma': trial.suggest_categorical('gamma', [True, False]),\n",
    "        'gamma_rate': trial.suggest_uniform('gamma_rate', 0.9, 0.99),\n",
    "        'optimizer': trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'ASGD', 'LBFGS'])\n",
    "    }\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Run each trial 5 times\n",
    "    for run in range(5):\n",
    "        model = get_resnet_model(params['model_depth'], params['dropout']).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if params['optimizer'] == 'Adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'SGD':\n",
    "            optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'], momentum=0.9)\n",
    "        elif params['optimizer'] == 'ASGD':\n",
    "            optimizer = optim.ASGD(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'LBFGS':\n",
    "            optimizer = optim.LBFGS(model.parameters(), lr=params['learning_rate'])  # Removed weight_decay\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer type\")\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=params['gamma_rate']) if params['gamma'] else None\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        for epoch in range(25):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "                \n",
    "                if params['optimizer'] == 'LBFGS':\n",
    "                    optimizer.step(closure)  # Requires closure function\n",
    "                else:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "    # Calculate statistics\n",
    "    min_acc = np.min(val_accuracies)\n",
    "    max_acc = np.max(val_accuracies)\n",
    "    avg_acc = np.mean(val_accuracies)\n",
    "    std_acc = np.std(val_accuracies)\n",
    "    \n",
    "    # Report all statistics\n",
    "    trial.set_user_attr(\"min_accuracy\", min_acc)\n",
    "    trial.set_user_attr(\"max_accuracy\", max_acc)\n",
    "    trial.set_user_attr(\"avg_accuracy\", avg_acc)\n",
    "    trial.set_user_attr(\"std_accuracy\", std_acc)\n",
    "    \n",
    "    # Print statistics for each trial\n",
    "    print(f\"\\nTrial {trial.number} - Results:\")\n",
    "    print(f\"  Parameters: {params}\")\n",
    "    print(f\"  Min Accuracy: {min_acc:.2f}%\")\n",
    "    print(f\"  Max Accuracy: {max_acc:.2f}%\")\n",
    "    print(f\"  Avg Accuracy: {avg_acc:.2f}%\")\n",
    "    print(f\"  Std Dev Accuracy: {std_acc:.2f}%\\n\")\n",
    "    \n",
    "    # Optuna will optimize for average accuracy\n",
    "    return -avg_acc  # Minimizing negative accuracy to maximize positive accuracy\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Display results\n",
    "best_trial = study.best_trial\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Value: {-best_trial.value}\")  # Converting back to positive accuracy\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Display additional statistics\n",
    "print(\"\\n  Additional statistics: \")\n",
    "print(f\"    Min Accuracy: {best_trial.user_attrs['min_accuracy']:.2f}%\")\n",
    "\n",
    "print(f\"    Max Accuracy: {best_trial.user_attrs['max_accuracy']:.2f}%\")\n",
    "print(f\"    Avg Accuracy: {best_trial.user_attrs['avg_accuracy']:.2f}%\")\n",
    "print(f\"    Std Accuracy: {best_trial.user_attrs['std_accuracy']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-26 13:23:30,587] A new study created in memory with name: no-name-2694195e-e012-4c12-983a-1ca102ad46e6\n",
      "C:\\Users\\Colem\\AppData\\Local\\Temp\\ipykernel_24320\\2031612572.py:63: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-2),\n",
      "C:\\Users\\Colem\\AppData\\Local\\Temp\\ipykernel_24320\\2031612572.py:64: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-2),\n",
      "C:\\Users\\Colem\\AppData\\Local\\Temp\\ipykernel_24320\\2031612572.py:65: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'dropout': trial.suggest_uniform('dropout', 0, 0.9),\n",
      "C:\\Users\\Colem\\AppData\\Local\\Temp\\ipykernel_24320\\2031612572.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'gamma_rate': trial.suggest_uniform('gamma_rate', 0.9, 0.99),\n",
      "[I 2025-02-26 13:24:11,539] Trial 0 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 8.547875488748409e-05, 'weight_decay': 0.004577021458952194, 'dropout': 0.33511662088045124, 'gamma': True, 'gamma_rate': 0.931679512729067, 'optimizer': 'Adam'}. Best is trial 0 with value: -75.51020408163265.\n",
      "[I 2025-02-26 13:25:13,914] Trial 1 finished with value: -67.34693877551021 and parameters: {'model_depth': 34, 'learning_rate': 0.00020610182798202063, 'weight_decay': 1.5421379910449017e-06, 'dropout': 0.7913000096311735, 'gamma': True, 'gamma_rate': 0.9048112880376933, 'optimizer': 'SGD'}. Best is trial 0 with value: -75.51020408163265.\n",
      "[I 2025-02-26 13:25:58,530] Trial 2 finished with value: -26.53061224489796 and parameters: {'model_depth': 18, 'learning_rate': 0.00015419642302823887, 'weight_decay': 0.002565801731104884, 'dropout': 0.7718926637712593, 'gamma': True, 'gamma_rate': 0.9760646381929972, 'optimizer': 'ASGD'}. Best is trial 0 with value: -75.51020408163265.\n",
      "[I 2025-02-26 13:26:39,266] Trial 3 finished with value: -71.42857142857143 and parameters: {'model_depth': 18, 'learning_rate': 0.0013579780052384955, 'weight_decay': 0.0017341523859443534, 'dropout': 0.6950941907406358, 'gamma': True, 'gamma_rate': 0.9368918364233019, 'optimizer': 'Adam'}. Best is trial 0 with value: -75.51020408163265.\n",
      "[I 2025-02-26 13:27:42,888] Trial 4 finished with value: -73.46938775510205 and parameters: {'model_depth': 34, 'learning_rate': 4.8227401636872385e-05, 'weight_decay': 6.985493959220513e-06, 'dropout': 0.540843507793519, 'gamma': True, 'gamma_rate': 0.9493115633215757, 'optimizer': 'Adam'}. Best is trial 0 with value: -75.51020408163265.\n",
      "[I 2025-02-26 13:28:46,153] Trial 5 finished with value: -69.38775510204081 and parameters: {'model_depth': 34, 'learning_rate': 3.689901038672941e-05, 'weight_decay': 0.001198859643137707, 'dropout': 0.42983332245332795, 'gamma': False, 'gamma_rate': 0.9071205614709562, 'optimizer': 'SGD'}. Best is trial 0 with value: -75.51020408163265.\n",
      "[I 2025-02-26 13:30:02,941] Trial 6 finished with value: -28.571428571428573 and parameters: {'model_depth': 18, 'learning_rate': 0.0016807239625806524, 'weight_decay': 0.001571367320535124, 'dropout': 0.32973556834838796, 'gamma': True, 'gamma_rate': 0.9735858659128823, 'optimizer': 'LBFGS'}. Best is trial 0 with value: -75.51020408163265.\n",
      "[I 2025-02-26 13:31:10,775] Trial 7 finished with value: -71.42857142857143 and parameters: {'model_depth': 34, 'learning_rate': 0.0027594658979666316, 'weight_decay': 8.399546376656577e-05, 'dropout': 0.273483299863044, 'gamma': False, 'gamma_rate': 0.976684658275526, 'optimizer': 'ASGD'}. Best is trial 0 with value: -75.51020408163265.\n",
      "[I 2025-02-26 13:31:51,472] Trial 8 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 0.0009453750428912866, 'weight_decay': 3.0393904276185313e-05, 'dropout': 0.4242408277322211, 'gamma': False, 'gamma_rate': 0.9675166118597753, 'optimizer': 'SGD'}. Best is trial 0 with value: -75.51020408163265.\n",
      "[I 2025-02-26 13:32:36,063] Trial 9 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.006817962106805295, 'weight_decay': 7.083571935901171e-05, 'dropout': 0.1667018626686459, 'gamma': True, 'gamma_rate': 0.9646587873391702, 'optimizer': 'ASGD'}. Best is trial 9 with value: -77.55102040816327.\n",
      "[I 2025-02-26 13:33:20,481] Trial 10 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 0.00968095607941788, 'weight_decay': 0.00020568344258872531, 'dropout': 0.05349736625572135, 'gamma': False, 'gamma_rate': 0.9545305180988828, 'optimizer': 'ASGD'}. Best is trial 9 with value: -77.55102040816327.\n",
      "[I 2025-02-26 13:34:01,306] Trial 11 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 1.3529361947151796e-05, 'weight_decay': 0.009244543792759708, 'dropout': 0.10727550119422657, 'gamma': True, 'gamma_rate': 0.9282135325466114, 'optimizer': 'Adam'}. Best is trial 9 with value: -77.55102040816327.\n",
      "[I 2025-02-26 13:35:25,364] Trial 12 finished with value: -26.53061224489796 and parameters: {'model_depth': 18, 'learning_rate': 0.009975839441058504, 'weight_decay': 0.000243680829059487, 'dropout': 0.19914855772381665, 'gamma': True, 'gamma_rate': 0.923811259400738, 'optimizer': 'LBFGS'}. Best is trial 9 with value: -77.55102040816327.\n",
      "[I 2025-02-26 13:36:06,246] Trial 13 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 0.0004899385378421406, 'weight_decay': 1.788020015182517e-05, 'dropout': 0.16750279849920446, 'gamma': True, 'gamma_rate': 0.9898217978261353, 'optimizer': 'Adam'}. Best is trial 9 with value: -77.55102040816327.\n",
      "[I 2025-02-26 13:36:51,899] Trial 14 finished with value: -32.6530612244898 and parameters: {'model_depth': 18, 'learning_rate': 7.995513382951507e-05, 'weight_decay': 0.00040170750828541897, 'dropout': 0.5561032254141098, 'gamma': True, 'gamma_rate': 0.9567412818595064, 'optimizer': 'ASGD'}. Best is trial 9 with value: -77.55102040816327.\n",
      "[I 2025-02-26 13:37:28,170] Trial 15 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 0.00040829591409329485, 'weight_decay': 0.008591393756703735, 'dropout': 0.2874575661082525, 'gamma': True, 'gamma_rate': 0.9394518661258527, 'optimizer': 'Adam'}. Best is trial 15 with value: -79.59183673469387.\n",
      "[I 2025-02-26 13:38:07,062] Trial 16 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 0.0038437340565971793, 'weight_decay': 4.556826678607156e-05, 'dropout': 0.007939922697477564, 'gamma': True, 'gamma_rate': 0.9414498667229013, 'optimizer': 'ASGD'}. Best is trial 15 with value: -79.59183673469387.\n",
      "[I 2025-02-26 13:39:12,954] Trial 17 finished with value: -10.204081632653061 and parameters: {'model_depth': 18, 'learning_rate': 0.000594393745108865, 'weight_decay': 7.266425803101806e-06, 'dropout': 0.21766210139380032, 'gamma': True, 'gamma_rate': 0.9187087451673236, 'optimizer': 'LBFGS'}. Best is trial 15 with value: -79.59183673469387.\n",
      "[I 2025-02-26 13:40:11,850] Trial 18 finished with value: -71.42857142857143 and parameters: {'model_depth': 34, 'learning_rate': 0.003755962175112615, 'weight_decay': 0.0006026433392985876, 'dropout': 0.12915069281654584, 'gamma': False, 'gamma_rate': 0.962584414314876, 'optimizer': 'ASGD'}. Best is trial 15 with value: -79.59183673469387.\n",
      "[I 2025-02-26 13:40:47,332] Trial 19 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 1.3629503598435713e-05, 'weight_decay': 1.0069085017760428e-06, 'dropout': 0.34563983031447715, 'gamma': True, 'gamma_rate': 0.9453912340935835, 'optimizer': 'Adam'}. Best is trial 15 with value: -79.59183673469387.\n",
      "[I 2025-02-26 13:41:22,785] Trial 20 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 1.8026264400630428e-05, 'weight_decay': 1.5707953051366504e-06, 'dropout': 0.556160333079368, 'gamma': True, 'gamma_rate': 0.9442211675808287, 'optimizer': 'Adam'}. Best is trial 15 with value: -79.59183673469387.\n",
      "[I 2025-02-26 13:41:58,262] Trial 21 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 1.0132749134408373e-05, 'weight_decay': 1.0754339592516492e-06, 'dropout': 0.556845299967037, 'gamma': True, 'gamma_rate': 0.9454330059511244, 'optimizer': 'Adam'}. Best is trial 15 with value: -79.59183673469387.\n",
      "[I 2025-02-26 13:42:33,769] Trial 22 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 2.1842132687411543e-05, 'weight_decay': 2.6033307782418793e-06, 'dropout': 0.6659004507308511, 'gamma': True, 'gamma_rate': 0.9372364290350421, 'optimizer': 'Adam'}. Best is trial 15 with value: -79.59183673469387.\n",
      "[I 2025-02-26 13:43:09,226] Trial 23 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 2.221196754870925e-05, 'weight_decay': 3.948669959061506e-06, 'dropout': 0.3598704122086194, 'gamma': True, 'gamma_rate': 0.9158525987769696, 'optimizer': 'Adam'}. Best is trial 15 with value: -79.59183673469387.\n",
      "[I 2025-02-26 13:43:44,678] Trial 24 finished with value: -69.38775510204081 and parameters: {'model_depth': 18, 'learning_rate': 0.00027937270296040707, 'weight_decay': 1.3157974560473889e-05, 'dropout': 0.887380236654331, 'gamma': True, 'gamma_rate': 0.9488797900528888, 'optimizer': 'Adam'}. Best is trial 15 with value: -79.59183673469387.\n",
      "[I 2025-02-26 13:44:20,160] Trial 25 finished with value: -85.71428571428571 and parameters: {'model_depth': 18, 'learning_rate': 0.00010693471395379046, 'weight_decay': 2.366898587557215e-06, 'dropout': 0.4955157772921656, 'gamma': True, 'gamma_rate': 0.9401941147532752, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:45:15,133] Trial 26 finished with value: -77.55102040816327 and parameters: {'model_depth': 34, 'learning_rate': 0.00010527449973850734, 'weight_decay': 3.471585542596849e-06, 'dropout': 0.4920181005904656, 'gamma': False, 'gamma_rate': 0.9333424137570842, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:45:50,587] Trial 27 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 0.0004177835115130776, 'weight_decay': 8.359291773854918e-06, 'dropout': 0.29007406832571525, 'gamma': True, 'gamma_rate': 0.9558641748914644, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:46:26,045] Trial 28 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 0.00041284486363686796, 'weight_decay': 9.396929221013983e-06, 'dropout': 0.25232396196972623, 'gamma': True, 'gamma_rate': 0.9548899048732427, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:47:01,522] Trial 29 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.0007519602298897357, 'weight_decay': 9.431050883909055e-06, 'dropout': 0.24998841819161327, 'gamma': True, 'gamma_rate': 0.956096735961168, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:47:36,947] Trial 30 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.0002877749584577371, 'weight_decay': 2.2508902261429068e-05, 'dropout': 0.37826256736195035, 'gamma': True, 'gamma_rate': 0.9826840588658932, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:48:12,432] Trial 31 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 0.0004199488774749085, 'weight_decay': 5.540942342514022e-06, 'dropout': 0.2804087128701197, 'gamma': True, 'gamma_rate': 0.953795200710511, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:48:47,868] Trial 32 finished with value: -71.42857142857143 and parameters: {'model_depth': 18, 'learning_rate': 0.00016925934352253605, 'weight_decay': 1.8703991971500875e-06, 'dropout': 0.3089771340900581, 'gamma': True, 'gamma_rate': 0.9313178948437064, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:49:23,174] Trial 33 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 0.00010410245041567233, 'weight_decay': 0.004274782563778496, 'dropout': 0.47516195297435454, 'gamma': True, 'gamma_rate': 0.9613493881197418, 'optimizer': 'SGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:49:58,622] Trial 34 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 0.0002214364275337602, 'weight_decay': 3.757736232038227e-05, 'dropout': 0.3994391973933649, 'gamma': True, 'gamma_rate': 0.9388804216002978, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:50:53,586] Trial 35 finished with value: -63.265306122448976 and parameters: {'model_depth': 34, 'learning_rate': 0.001058741700197244, 'weight_decay': 1.2062957077659821e-05, 'dropout': 0.6262360740219856, 'gamma': True, 'gamma_rate': 0.9497794500432507, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:51:29,033] Trial 36 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 5.701663651352748e-05, 'weight_decay': 4.4848343725820535e-06, 'dropout': 0.2525080773900395, 'gamma': True, 'gamma_rate': 0.9695570474703454, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:52:34,354] Trial 37 finished with value: -6.122448979591836 and parameters: {'model_depth': 18, 'learning_rate': 0.0001369824229522825, 'weight_decay': 0.00013119827212602423, 'dropout': 0.49190846934659066, 'gamma': True, 'gamma_rate': 0.9588974270320254, 'optimizer': 'LBFGS'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:53:29,074] Trial 38 finished with value: -77.55102040816327 and parameters: {'model_depth': 34, 'learning_rate': 0.0003670846598401718, 'weight_decay': 2.598593829342704e-06, 'dropout': 0.30565179804427556, 'gamma': False, 'gamma_rate': 0.9004383985229415, 'optimizer': 'SGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:53:34,896] Trial 39 pruned. \n",
      "[I 2025-02-26 13:54:29,840] Trial 40 finished with value: -61.224489795918366 and parameters: {'model_depth': 34, 'learning_rate': 0.0018575051297719531, 'weight_decay': 0.0006958991625548342, 'dropout': 0.7508681178004765, 'gamma': True, 'gamma_rate': 0.9266663302599839, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:55:05,307] Trial 41 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 0.0006541727572874018, 'weight_decay': 1.1525965025383974e-06, 'dropout': 0.33896781337348064, 'gamma': True, 'gamma_rate': 0.9446885900155891, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:55:40,759] Trial 42 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 3.084097572128161e-05, 'weight_decay': 1.993784593520437e-06, 'dropout': 0.39419936935138866, 'gamma': True, 'gamma_rate': 0.9361855697991842, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:56:16,214] Trial 43 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 5.768260593300385e-05, 'weight_decay': 7.139037116017951e-06, 'dropout': 0.20555541721583465, 'gamma': True, 'gamma_rate': 0.9428438303123162, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:56:51,516] Trial 44 finished with value: -57.142857142857146 and parameters: {'model_depth': 18, 'learning_rate': 5.203645948020259e-05, 'weight_decay': 6.932739419405904e-06, 'dropout': 0.12625203021115797, 'gamma': True, 'gamma_rate': 0.9417719049300187, 'optimizer': 'SGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:57:26,953] Trial 45 finished with value: -61.224489795918366 and parameters: {'model_depth': 18, 'learning_rate': 0.00016898566371412963, 'weight_decay': 1.595017894393701e-05, 'dropout': 0.2144870095387184, 'gamma': False, 'gamma_rate': 0.9325586075519202, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:58:32,863] Trial 46 finished with value: -32.6530612244898 and parameters: {'model_depth': 18, 'learning_rate': 7.388870075411574e-05, 'weight_decay': 9.993006807724842e-06, 'dropout': 0.08194124827365781, 'gamma': True, 'gamma_rate': 0.9515429665894662, 'optimizer': 'LBFGS'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:59:08,327] Trial 47 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 3.972681166353095e-05, 'weight_decay': 2.817899045479618e-05, 'dropout': 0.18495957895084353, 'gamma': True, 'gamma_rate': 0.9487063827909735, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 13:59:43,769] Trial 48 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 0.0004776293858920766, 'weight_decay': 5.893929556096862e-05, 'dropout': 0.24061759121964663, 'gamma': True, 'gamma_rate': 0.9665873546762235, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:00:19,217] Trial 49 finished with value: -69.38775510204081 and parameters: {'model_depth': 18, 'learning_rate': 0.0009031016751007457, 'weight_decay': 2.81032029527852e-06, 'dropout': 0.15464994718917394, 'gamma': False, 'gamma_rate': 0.940766668272228, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:00:54,528] Trial 50 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 0.0012666851946671071, 'weight_decay': 0.00014341536192446304, 'dropout': 0.3044669200509676, 'gamma': True, 'gamma_rate': 0.97135154273073, 'optimizer': 'SGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:01:29,995] Trial 51 finished with value: -83.6734693877551 and parameters: {'model_depth': 18, 'learning_rate': 0.00012285883707282191, 'weight_decay': 5.629898672148179e-06, 'dropout': 0.345494033014042, 'gamma': True, 'gamma_rate': 0.9471480542674159, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:02:05,452] Trial 52 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 0.0001059743645979451, 'weight_decay': 5.690439636588184e-06, 'dropout': 0.2775820909218194, 'gamma': True, 'gamma_rate': 0.9586858684204451, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:02:40,903] Trial 53 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 0.00032872763084289876, 'weight_decay': 7.63734332858816e-06, 'dropout': 0.42043693220846123, 'gamma': True, 'gamma_rate': 0.9466415691736645, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:03:19,783] Trial 54 finished with value: -40.816326530612244 and parameters: {'model_depth': 18, 'learning_rate': 0.00013608809094457425, 'weight_decay': 0.0023642622990973088, 'dropout': 0.20407231395268716, 'gamma': True, 'gamma_rate': 0.9354734414660789, 'optimizer': 'ASGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:03:55,237] Trial 55 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.0005566713905982422, 'weight_decay': 2.0297520479737633e-05, 'dropout': 0.31375039673852145, 'gamma': True, 'gamma_rate': 0.9292393421479406, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:05:00,948] Trial 56 finished with value: -24.489795918367346 and parameters: {'model_depth': 18, 'learning_rate': 6.717845049168526e-05, 'weight_decay': 4.825095300450373e-06, 'dropout': 0.5991951039566943, 'gamma': True, 'gamma_rate': 0.9536847347456697, 'optimizer': 'LBFGS'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:05:55,949] Trial 57 finished with value: -73.46938775510205 and parameters: {'model_depth': 34, 'learning_rate': 0.00024499388814307237, 'weight_decay': 3.3070835118670438e-06, 'dropout': 0.3639478137298132, 'gamma': True, 'gamma_rate': 0.9400459793503495, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:06:31,401] Trial 58 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.00041152030808914715, 'weight_decay': 9.420876874195948e-06, 'dropout': 0.2385518965407867, 'gamma': True, 'gamma_rate': 0.9235410504248083, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:07:06,889] Trial 59 finished with value: -85.71428571428571 and parameters: {'model_depth': 18, 'learning_rate': 4.078098738614338e-05, 'weight_decay': 0.0012612894829566095, 'dropout': 0.14649656393781918, 'gamma': False, 'gamma_rate': 0.9623254327705761, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:07:45,771] Trial 60 finished with value: -24.489795918367346 and parameters: {'model_depth': 18, 'learning_rate': 3.5108038457707406e-05, 'weight_decay': 0.0012955911480440011, 'dropout': 0.06408194506259249, 'gamma': False, 'gamma_rate': 0.9635890426331984, 'optimizer': 'ASGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:08:21,227] Trial 61 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 8.899159068855273e-05, 'weight_decay': 0.004058249789086598, 'dropout': 0.1502009768477591, 'gamma': False, 'gamma_rate': 0.9596523990975949, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:08:56,695] Trial 62 finished with value: -71.42857142857143 and parameters: {'model_depth': 18, 'learning_rate': 2.559940177936726e-05, 'weight_decay': 0.0069980009714543856, 'dropout': 0.015431756147107012, 'gamma': False, 'gamma_rate': 0.9537400298145273, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:09:32,148] Trial 63 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 0.00015847000333146677, 'weight_decay': 0.0008448372148528327, 'dropout': 0.18005984948266882, 'gamma': False, 'gamma_rate': 0.9430997900072106, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:10:07,600] Trial 64 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 4.404955630108498e-05, 'weight_decay': 0.002446299052064468, 'dropout': 0.10167104065424926, 'gamma': False, 'gamma_rate': 0.947250930855718, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:10:43,035] Trial 65 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.0002012716291051795, 'weight_decay': 0.0004189941101516941, 'dropout': 0.2705046396912446, 'gamma': True, 'gamma_rate': 0.9659214253592326, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:11:18,473] Trial 66 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 0.00011515013844813414, 'weight_decay': 0.00027970375369147394, 'dropout': 0.22100050330470128, 'gamma': True, 'gamma_rate': 0.9568926908331202, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:11:53,929] Trial 67 finished with value: -69.38775510204081 and parameters: {'model_depth': 18, 'learning_rate': 0.00011689217661759611, 'weight_decay': 0.00030156883651335077, 'dropout': 0.20887045577793617, 'gamma': True, 'gamma_rate': 0.9753051741702385, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:12:29,399] Trial 68 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 6.187994355433779e-05, 'weight_decay': 2.248078918962272e-06, 'dropout': 0.12359904067610335, 'gamma': False, 'gamma_rate': 0.9563220337323115, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:13:24,372] Trial 69 finished with value: -75.51020408163265 and parameters: {'model_depth': 34, 'learning_rate': 9.02404419659708e-05, 'weight_decay': 1.3206791480393153e-05, 'dropout': 0.468489194800979, 'gamma': True, 'gamma_rate': 0.9795001343658281, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:14:30,103] Trial 70 finished with value: -20.408163265306122 and parameters: {'model_depth': 18, 'learning_rate': 5.2770278732365176e-05, 'weight_decay': 9.79513831296282e-05, 'dropout': 0.523578599288177, 'gamma': True, 'gamma_rate': 0.9619025579147599, 'optimizer': 'LBFGS'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:15:05,554] Trial 71 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.0002919579803597715, 'weight_decay': 1.497178809082351e-06, 'dropout': 0.2348156622184474, 'gamma': True, 'gamma_rate': 0.9507102860914279, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:15:41,008] Trial 72 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 8.173887787764655e-05, 'weight_decay': 0.005776649759973369, 'dropout': 0.33054797376677864, 'gamma': True, 'gamma_rate': 0.9570998197696831, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:16:16,460] Trial 73 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 0.0007525427582147627, 'weight_decay': 0.003383783494109546, 'dropout': 0.26665580400479616, 'gamma': True, 'gamma_rate': 0.9436282207305191, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:16:51,925] Trial 74 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 0.00018857568242918303, 'weight_decay': 0.0011339159275391391, 'dropout': 0.1841139981588662, 'gamma': True, 'gamma_rate': 0.9526239846033183, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:17:27,364] Trial 75 finished with value: -71.42857142857143 and parameters: {'model_depth': 18, 'learning_rate': 0.00012341098026480906, 'weight_decay': 0.0017952553033787225, 'dropout': 0.29375505135484953, 'gamma': True, 'gamma_rate': 0.9381061859258342, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:18:02,663] Trial 76 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.00035717250392906563, 'weight_decay': 0.00020031911033103407, 'dropout': 0.3610806301292932, 'gamma': True, 'gamma_rate': 0.9483639640182537, 'optimizer': 'SGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:18:41,526] Trial 77 finished with value: -4.081632653061225 and parameters: {'model_depth': 18, 'learning_rate': 2.8005158545098037e-05, 'weight_decay': 6.0249346679470245e-06, 'dropout': 0.4052304798570503, 'gamma': True, 'gamma_rate': 0.9602207733683167, 'optimizer': 'ASGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:19:16,974] Trial 78 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.0005095409677665227, 'weight_decay': 0.0004729552919475344, 'dropout': 0.1515922751907843, 'gamma': True, 'gamma_rate': 0.9555561638381526, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:20:11,968] Trial 79 finished with value: -67.34693877551021 and parameters: {'model_depth': 34, 'learning_rate': 0.0002451373625604914, 'weight_decay': 3.55275891521785e-06, 'dropout': 0.3313738291593703, 'gamma': False, 'gamma_rate': 0.9694046698442109, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:20:47,439] Trial 80 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 1.6659887622240155e-05, 'weight_decay': 0.00029673288641103075, 'dropout': 0.22946485008672246, 'gamma': True, 'gamma_rate': 0.9646483653903926, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:21:22,912] Trial 81 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 1.6461317617414022e-05, 'weight_decay': 1.419648481087828e-06, 'dropout': 0.3472367336340211, 'gamma': True, 'gamma_rate': 0.9453845208430937, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:21:58,360] Trial 82 finished with value: -71.42857142857143 and parameters: {'model_depth': 18, 'learning_rate': 2.141921656788183e-05, 'weight_decay': 1.3198698721839229e-06, 'dropout': 0.28682404844380216, 'gamma': True, 'gamma_rate': 0.9420405828915476, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:22:33,828] Trial 83 finished with value: -67.34693877551021 and parameters: {'model_depth': 18, 'learning_rate': 0.00043013006133070574, 'weight_decay': 1.0289385171857624e-06, 'dropout': 0.44510900554362726, 'gamma': True, 'gamma_rate': 0.9459841777349033, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:23:09,292] Trial 84 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 1.3449999894513264e-05, 'weight_decay': 2.5906056628044145e-05, 'dropout': 0.25549715224698866, 'gamma': True, 'gamma_rate': 0.9335576922498522, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:23:44,771] Trial 85 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 1.1739437211336e-05, 'weight_decay': 2.1033209580941382e-06, 'dropout': 0.3098909813408276, 'gamma': True, 'gamma_rate': 0.9389354840037577, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:24:20,228] Trial 86 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 4.659575844445371e-05, 'weight_decay': 4.123856579152272e-05, 'dropout': 0.5330804250713058, 'gamma': True, 'gamma_rate': 0.9502597351349826, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:24:55,687] Trial 87 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 0.0006715065613806856, 'weight_decay': 4.7457938283606535e-06, 'dropout': 0.21379355546481782, 'gamma': True, 'gamma_rate': 0.9352427612090176, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:26:01,423] Trial 88 finished with value: -18.367346938775512 and parameters: {'model_depth': 18, 'learning_rate': 6.677334375137405e-05, 'weight_decay': 8.512513092587505e-06, 'dropout': 0.38015358192312704, 'gamma': True, 'gamma_rate': 0.9579114233890739, 'optimizer': 'LBFGS'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:26:36,741] Trial 89 finished with value: -44.89795918367347 and parameters: {'model_depth': 18, 'learning_rate': 3.279392250251856e-05, 'weight_decay': 1.1152747593813096e-05, 'dropout': 0.1721789604086578, 'gamma': True, 'gamma_rate': 0.9488095222618476, 'optimizer': 'SGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:27:12,197] Trial 90 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 3.915206042279548e-05, 'weight_decay': 1.6896142429481395e-05, 'dropout': 0.5001780402528491, 'gamma': False, 'gamma_rate': 0.9550051643950097, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:27:47,653] Trial 91 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 4.0718102106613114e-05, 'weight_decay': 1.4746211663021324e-05, 'dropout': 0.49915713767714043, 'gamma': False, 'gamma_rate': 0.9527882581237591, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:28:23,094] Trial 92 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 0.00010233660639701307, 'weight_decay': 6.073602535601207e-05, 'dropout': 0.46238403333966654, 'gamma': False, 'gamma_rate': 0.954885217063483, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:28:58,558] Trial 93 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 7.393706484528412e-05, 'weight_decay': 4.078857490542413e-06, 'dropout': 0.4279713298363902, 'gamma': False, 'gamma_rate': 0.9435196762961031, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:29:34,006] Trial 94 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 5.939911981077537e-05, 'weight_decay': 6.3983759091238915e-06, 'dropout': 0.592151708847522, 'gamma': False, 'gamma_rate': 0.9434083863209518, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:30:09,468] Trial 95 finished with value: -83.6734693877551 and parameters: {'model_depth': 18, 'learning_rate': 6.969020503126633e-05, 'weight_decay': 4.2027743106370865e-06, 'dropout': 0.4982655333868919, 'gamma': False, 'gamma_rate': 0.9627191746374625, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:30:44,934] Trial 96 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 0.00013847966616994546, 'weight_decay': 7.317230102205111e-06, 'dropout': 0.5019538050985305, 'gamma': False, 'gamma_rate': 0.9613237460845441, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:31:39,940] Trial 97 finished with value: -77.55102040816327 and parameters: {'model_depth': 34, 'learning_rate': 7.358300180268759e-05, 'weight_decay': 4.094761165923285e-06, 'dropout': 0.4311714388854744, 'gamma': False, 'gamma_rate': 0.9683984165139825, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:32:18,827] Trial 98 finished with value: -22.448979591836736 and parameters: {'model_depth': 18, 'learning_rate': 4.766909900471195e-05, 'weight_decay': 2.7939254027256905e-06, 'dropout': 0.5911213042135954, 'gamma': False, 'gamma_rate': 0.9634954192669533, 'optimizer': 'ASGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:32:54,291] Trial 99 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 9.226238089453388e-05, 'weight_decay': 3.2426070891095034e-06, 'dropout': 0.5218299804395338, 'gamma': False, 'gamma_rate': 0.958045496080925, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:33:29,747] Trial 100 finished with value: -71.42857142857143 and parameters: {'model_depth': 18, 'learning_rate': 3.73693633141032e-05, 'weight_decay': 1.7707442683671067e-05, 'dropout': 0.4792416508926023, 'gamma': False, 'gamma_rate': 0.9714019791365603, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:34:05,225] Trial 101 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 5.726029463135817e-05, 'weight_decay': 5.0706446998456445e-06, 'dropout': 0.19346013661396413, 'gamma': False, 'gamma_rate': 0.9598389390962865, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:34:40,693] Trial 102 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 7.823485900052995e-05, 'weight_decay': 8.5113625832911e-06, 'dropout': 0.5627297888193835, 'gamma': False, 'gamma_rate': 0.9410408511718995, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:35:16,169] Trial 103 finished with value: -69.38775510204081 and parameters: {'model_depth': 18, 'learning_rate': 0.00027197966841606424, 'weight_decay': 1.2017935060383932e-05, 'dropout': 0.5112000250999115, 'gamma': False, 'gamma_rate': 0.9472818062902826, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:35:51,631] Trial 104 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.00011873495516129148, 'weight_decay': 1.7882231266549347e-06, 'dropout': 0.42217187370057996, 'gamma': False, 'gamma_rate': 0.9375181566456842, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:36:27,082] Trial 105 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 0.00017827583517696595, 'weight_decay': 4.0883087448689235e-06, 'dropout': 0.2527214012932587, 'gamma': False, 'gamma_rate': 0.9653324230177901, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:37:02,545] Trial 106 finished with value: -71.42857142857143 and parameters: {'model_depth': 18, 'learning_rate': 0.0003444055372005295, 'weight_decay': 2.410576559595961e-06, 'dropout': 0.5721080282530596, 'gamma': False, 'gamma_rate': 0.9400431761874559, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:37:37,998] Trial 107 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 5.135358650100922e-05, 'weight_decay': 5.7316198708153e-06, 'dropout': 0.3826676409333294, 'gamma': False, 'gamma_rate': 0.952416021507514, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:38:39,611] Trial 108 finished with value: -40.816326530612244 and parameters: {'model_depth': 18, 'learning_rate': 6.898288497466585e-05, 'weight_decay': 0.0007782609186867169, 'dropout': 0.4114885182158378, 'gamma': False, 'gamma_rate': 0.9555293358532574, 'optimizer': 'LBFGS'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:38:45,431] Trial 109 pruned. \n",
      "[I 2025-02-26 14:39:40,435] Trial 110 finished with value: -75.51020408163265 and parameters: {'model_depth': 34, 'learning_rate': 9.983405345629503e-05, 'weight_decay': 3.175159994696759e-06, 'dropout': 0.5405926502397929, 'gamma': False, 'gamma_rate': 0.961895337072126, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:40:15,910] Trial 111 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 0.0004229225614449023, 'weight_decay': 0.009762798812836669, 'dropout': 0.349930353880008, 'gamma': True, 'gamma_rate': 0.9448788531341104, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:40:51,369] Trial 112 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 8.27433547233819e-05, 'weight_decay': 9.93121049182138e-06, 'dropout': 0.45800088136589084, 'gamma': True, 'gamma_rate': 0.9422451440379424, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:41:26,828] Trial 113 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 1.9665345877920172e-05, 'weight_decay': 7.113818841049256e-06, 'dropout': 0.32449065746324934, 'gamma': True, 'gamma_rate': 0.9301684607791576, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:42:02,290] Trial 114 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.00014252712776081588, 'weight_decay': 4.900557005370203e-06, 'dropout': 0.2962318305277834, 'gamma': True, 'gamma_rate': 0.9478811238545586, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:42:37,611] Trial 115 finished with value: -44.89795918367347 and parameters: {'model_depth': 18, 'learning_rate': 2.444983176690613e-05, 'weight_decay': 0.0005542605616574352, 'dropout': 0.26800215816176787, 'gamma': True, 'gamma_rate': 0.9351778542695083, 'optimizer': 'SGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:43:13,075] Trial 116 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 4.236058870570464e-05, 'weight_decay': 0.005167475461249281, 'dropout': 0.8546803451751362, 'gamma': True, 'gamma_rate': 0.9565986052840281, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:43:48,544] Trial 117 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 0.0006223786237482354, 'weight_decay': 1.2533021065111369e-06, 'dropout': 0.3641419525060906, 'gamma': True, 'gamma_rate': 0.9543241101041265, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:44:27,411] Trial 118 finished with value: -61.224489795918366 and parameters: {'model_depth': 18, 'learning_rate': 0.00021145769366417085, 'weight_decay': 1.7577824248420053e-06, 'dropout': 0.48496729648167797, 'gamma': True, 'gamma_rate': 0.9899006454786158, 'optimizer': 'ASGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:45:02,866] Trial 119 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.0005150887501342763, 'weight_decay': 3.64008705900447e-06, 'dropout': 0.439873186540988, 'gamma': True, 'gamma_rate': 0.9390260662407942, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:45:39,436] Trial 120 finished with value: -69.38775510204081 and parameters: {'model_depth': 18, 'learning_rate': 0.00011497942633981947, 'weight_decay': 0.0001478605334077153, 'dropout': 0.631630896773422, 'gamma': False, 'gamma_rate': 0.9458470130778032, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:46:21,542] Trial 121 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 1.9285373111002017e-05, 'weight_decay': 1.7102058629018182e-06, 'dropout': 0.546566513611809, 'gamma': True, 'gamma_rate': 0.9432276678132654, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:47:00,993] Trial 122 finished with value: -71.42857142857143 and parameters: {'model_depth': 18, 'learning_rate': 1.5239781152369915e-05, 'weight_decay': 2.2546168933043853e-06, 'dropout': 0.2849563686415437, 'gamma': True, 'gamma_rate': 0.9502328126188477, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:47:40,435] Trial 123 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 1.0681170653803825e-05, 'weight_decay': 1.0063302363974408e-06, 'dropout': 0.24690606505896084, 'gamma': True, 'gamma_rate': 0.9438932777444939, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:48:19,908] Trial 124 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 1.3465841322966984e-05, 'weight_decay': 1.5550577631125304e-06, 'dropout': 0.32322527268804424, 'gamma': True, 'gamma_rate': 0.9407477076358972, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:48:59,392] Trial 125 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 3.179613745744053e-05, 'weight_decay': 0.001987517693733944, 'dropout': 0.1502269050315646, 'gamma': True, 'gamma_rate': 0.9468266361812964, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:49:38,833] Trial 126 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 6.319383395762095e-05, 'weight_decay': 2.1572465815690202e-05, 'dropout': 0.514910935869392, 'gamma': True, 'gamma_rate': 0.9596658437152655, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:50:18,293] Trial 127 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 6.235309323245587e-05, 'weight_decay': 3.0972381425535336e-05, 'dropout': 0.39459528069975963, 'gamma': True, 'gamma_rate': 0.9603228437447576, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:50:57,758] Trial 128 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 5.3474989357109946e-05, 'weight_decay': 2.0489320849136965e-05, 'dropout': 0.5152931135131806, 'gamma': True, 'gamma_rate': 0.957963119388699, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:51:59,099] Trial 129 finished with value: -79.59183673469387 and parameters: {'model_depth': 34, 'learning_rate': 7.356835278004945e-05, 'weight_decay': 1.5084952951301325e-05, 'dropout': 0.09078296825555393, 'gamma': True, 'gamma_rate': 0.9624666170075702, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:53:11,967] Trial 130 finished with value: -6.122448979591836 and parameters: {'model_depth': 18, 'learning_rate': 9.26584893478665e-05, 'weight_decay': 0.0010493636913091581, 'dropout': 0.13057703572797186, 'gamma': False, 'gamma_rate': 0.9586178259275925, 'optimizer': 'LBFGS'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:53:51,423] Trial 131 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 3.7467160142174565e-05, 'weight_decay': 7.99633821587344e-06, 'dropout': 0.48683647532167557, 'gamma': True, 'gamma_rate': 0.9103874728655555, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:54:30,903] Trial 132 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 6.353993885220929e-05, 'weight_decay': 2.924653560216153e-06, 'dropout': 0.569293203230236, 'gamma': True, 'gamma_rate': 0.9492116846040121, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:55:10,366] Trial 133 finished with value: -83.6734693877551 and parameters: {'model_depth': 18, 'learning_rate': 6.321068479328795e-05, 'weight_decay': 1.0717065314634124e-05, 'dropout': 0.6172396003359745, 'gamma': True, 'gamma_rate': 0.9488541258824086, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:55:49,788] Trial 134 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 4.869784945822836e-05, 'weight_decay': 1.1052792847262553e-05, 'dropout': 0.6077446621353455, 'gamma': True, 'gamma_rate': 0.952528302795793, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:56:29,237] Trial 135 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 6.25593160050928e-05, 'weight_decay': 6.155920279907835e-06, 'dropout': 0.6771135370453173, 'gamma': True, 'gamma_rate': 0.9494365826449648, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:57:08,680] Trial 136 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 6.566449036556667e-05, 'weight_decay': 6.2188773997054215e-06, 'dropout': 0.7010004959084446, 'gamma': True, 'gamma_rate': 0.9489762052355429, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:57:48,091] Trial 137 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 5.717508811296974e-05, 'weight_decay': 1.351253894136269e-05, 'dropout': 0.6732080275807479, 'gamma': True, 'gamma_rate': 0.9541775987088457, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:58:27,540] Trial 138 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 8.177092187075807e-05, 'weight_decay': 9.463289578572719e-06, 'dropout': 0.6394200037673353, 'gamma': True, 'gamma_rate': 0.9513254525803517, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:59:06,819] Trial 139 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 4.345400480282057e-05, 'weight_decay': 4.461182416962071e-06, 'dropout': 0.6595706445457545, 'gamma': False, 'gamma_rate': 0.9561279294251324, 'optimizer': 'SGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 14:59:46,270] Trial 140 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 7.402375678765946e-05, 'weight_decay': 2.9866351154760507e-06, 'dropout': 0.6909955088148491, 'gamma': True, 'gamma_rate': 0.9644385845226378, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:00:25,737] Trial 141 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 8.114706699056547e-05, 'weight_decay': 9.6575878266714e-06, 'dropout': 0.7347635340561656, 'gamma': True, 'gamma_rate': 0.9516099787476435, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:01:05,186] Trial 142 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 9.382656080522529e-05, 'weight_decay': 1.739237001275183e-05, 'dropout': 0.6074419751208271, 'gamma': True, 'gamma_rate': 0.9485940075156479, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:01:44,645] Trial 143 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 6.319178475143704e-05, 'weight_decay': 8.229724426263858e-06, 'dropout': 0.5747267151780863, 'gamma': True, 'gamma_rate': 0.9668996691081418, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:02:24,105] Trial 144 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 8.33767298441333e-05, 'weight_decay': 5.726673141776909e-06, 'dropout': 0.6563903909758426, 'gamma': True, 'gamma_rate': 0.9515230529467262, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:03:03,542] Trial 145 finished with value: -83.6734693877551 and parameters: {'model_depth': 18, 'learning_rate': 5.283200337821571e-05, 'weight_decay': 6.711854024568056e-06, 'dropout': 0.6350879626324769, 'gamma': True, 'gamma_rate': 0.9542381635125292, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:03:42,975] Trial 146 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 4.9781800764471614e-05, 'weight_decay': 6.715572659419125e-06, 'dropout': 0.6200585865978668, 'gamma': True, 'gamma_rate': 0.954446429373226, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:04:22,410] Trial 147 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 3.8014645755839635e-05, 'weight_decay': 3.78572896194301e-06, 'dropout': 0.5330255954920305, 'gamma': False, 'gamma_rate': 0.9570511080770742, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:05:05,494] Trial 148 finished with value: -34.69387755102041 and parameters: {'model_depth': 18, 'learning_rate': 5.442763718624322e-05, 'weight_decay': 5.097650530586581e-06, 'dropout': 0.7069355257105018, 'gamma': True, 'gamma_rate': 0.953646707390834, 'optimizer': 'ASGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:05:44,978] Trial 149 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 6.271084043935352e-05, 'weight_decay': 2.4482892614106307e-05, 'dropout': 0.5857320069642258, 'gamma': True, 'gamma_rate': 0.9590326888853052, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:06:24,404] Trial 150 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 0.00011023558203043211, 'weight_decay': 1.1221973746922474e-05, 'dropout': 0.6433685256771587, 'gamma': False, 'gamma_rate': 0.9466764717481873, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:07:03,851] Trial 151 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 7.210053494915167e-05, 'weight_decay': 8.510860535221534e-06, 'dropout': 0.6441476250344816, 'gamma': True, 'gamma_rate': 0.9494289988690039, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:07:43,327] Trial 152 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 4.520577255437214e-05, 'weight_decay': 6.86335340254749e-06, 'dropout': 0.5507716237090123, 'gamma': True, 'gamma_rate': 0.9612087339014028, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:08:22,797] Trial 153 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.00012680646307496447, 'weight_decay': 1.2553706721095527e-05, 'dropout': 0.6157912150403836, 'gamma': True, 'gamma_rate': 0.9528589601394856, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:09:02,247] Trial 154 finished with value: -83.6734693877551 and parameters: {'model_depth': 18, 'learning_rate': 0.00010310945524905343, 'weight_decay': 4.33373920912058e-06, 'dropout': 0.6796566750620499, 'gamma': True, 'gamma_rate': 0.9516076536935834, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:09:41,697] Trial 155 finished with value: -83.6734693877551 and parameters: {'model_depth': 18, 'learning_rate': 0.00010255750848576387, 'weight_decay': 2.662832186017042e-06, 'dropout': 0.6828507956031733, 'gamma': True, 'gamma_rate': 0.9447353522629545, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:10:21,142] Trial 156 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 0.00015803267643959787, 'weight_decay': 2.6668321122666786e-06, 'dropout': 0.7282495927700534, 'gamma': True, 'gamma_rate': 0.9462854205418653, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:11:00,639] Trial 157 finished with value: -83.6734693877551 and parameters: {'model_depth': 18, 'learning_rate': 0.00010149745344013331, 'weight_decay': 4.283872288583009e-06, 'dropout': 0.19550980157684394, 'gamma': True, 'gamma_rate': 0.9443331789809477, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:12:03,924] Trial 158 finished with value: -63.265306122448976 and parameters: {'model_depth': 34, 'learning_rate': 0.00010838735768758287, 'weight_decay': 4.620883088366915e-06, 'dropout': 0.20149145438545707, 'gamma': False, 'gamma_rate': 0.944197084862531, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:12:42,193] Trial 159 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 0.00013346829917091805, 'weight_decay': 5.154706306827966e-06, 'dropout': 0.2241313110915894, 'gamma': True, 'gamma_rate': 0.955311098476862, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:13:21,701] Trial 160 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 9.965304676009035e-05, 'weight_decay': 3.5750564075817603e-06, 'dropout': 0.8030687100231448, 'gamma': True, 'gamma_rate': 0.9573840906104262, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:14:00,493] Trial 161 finished with value: -44.89795918367347 and parameters: {'model_depth': 18, 'learning_rate': 0.005713840004454475, 'weight_decay': 2.840441856850641e-06, 'dropout': 0.17130804922502846, 'gamma': True, 'gamma_rate': 0.9426134891934319, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:14:39,593] Trial 162 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 8.667257037162963e-05, 'weight_decay': 2.112919346859751e-06, 'dropout': 0.5119413449525302, 'gamma': True, 'gamma_rate': 0.9449964494795123, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:15:18,682] Trial 163 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 9.890255163410899e-05, 'weight_decay': 3.6169189253043216e-06, 'dropout': 0.4621046199650649, 'gamma': True, 'gamma_rate': 0.9601957286411328, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:15:57,765] Trial 164 finished with value: -69.38775510204081 and parameters: {'model_depth': 18, 'learning_rate': 6.923569038132692e-05, 'weight_decay': 2.346474559615918e-06, 'dropout': 0.19164638797274539, 'gamma': True, 'gamma_rate': 0.9414983302102423, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:16:36,869] Trial 165 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 0.000153533996450404, 'weight_decay': 4.320623988107767e-06, 'dropout': 0.4966435772401071, 'gamma': True, 'gamma_rate': 0.9472784235597792, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:17:15,916] Trial 166 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 0.00011986616387039614, 'weight_decay': 2.9390513989634094e-06, 'dropout': 0.5663874165230877, 'gamma': False, 'gamma_rate': 0.9504043980232136, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:17:55,025] Trial 167 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 3.3388757551144854e-05, 'weight_decay': 4.229600554352817e-06, 'dropout': 0.47644495047595736, 'gamma': True, 'gamma_rate': 0.9632151857408054, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:19:06,128] Trial 168 finished with value: -18.367346938775512 and parameters: {'model_depth': 18, 'learning_rate': 5.1530673504806426e-05, 'weight_decay': 5.303298347306211e-06, 'dropout': 0.13663056557510667, 'gamma': True, 'gamma_rate': 0.9564915285477286, 'optimizer': 'LBFGS'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:19:45,200] Trial 169 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 5.833613648253952e-05, 'weight_decay': 6.09194248072258e-06, 'dropout': 0.259316398395482, 'gamma': True, 'gamma_rate': 0.952731748972169, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:20:24,119] Trial 170 finished with value: -71.42857142857143 and parameters: {'model_depth': 18, 'learning_rate': 7.743710330444872e-05, 'weight_decay': 7.899162678273557e-05, 'dropout': 0.23568268672352563, 'gamma': False, 'gamma_rate': 0.9394506179750988, 'optimizer': 'SGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:21:03,211] Trial 171 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 6.085914764576635e-05, 'weight_decay': 7.654705409984738e-06, 'dropout': 0.6757151155010668, 'gamma': True, 'gamma_rate': 0.9485987954853713, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:21:42,332] Trial 172 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 6.799322491024472e-05, 'weight_decay': 5.916469960663466e-06, 'dropout': 0.7168205769026912, 'gamma': True, 'gamma_rate': 0.9445353272581193, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:22:21,409] Trial 173 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 4.134161508542224e-05, 'weight_decay': 7.078494733617561e-06, 'dropout': 0.6696706147739715, 'gamma': True, 'gamma_rate': 0.9500740998577163, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:23:00,533] Trial 174 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 8.653108685888948e-05, 'weight_decay': 3.216344712277315e-06, 'dropout': 0.6897658313664639, 'gamma': True, 'gamma_rate': 0.9550746612293107, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:23:39,626] Trial 175 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 4.565290328239365e-05, 'weight_decay': 1.0521043001458317e-05, 'dropout': 0.6523807344058831, 'gamma': True, 'gamma_rate': 0.9866831620220974, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:24:18,738] Trial 176 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.00010759666317704609, 'weight_decay': 3.959855925602407e-06, 'dropout': 0.6267211585023968, 'gamma': True, 'gamma_rate': 0.9476952346135764, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:24:57,826] Trial 177 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 5.646871278075607e-05, 'weight_decay': 1.4709421903209715e-05, 'dropout': 0.6809995576001674, 'gamma': True, 'gamma_rate': 0.945900683275538, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:25:37,006] Trial 178 finished with value: -69.38775510204081 and parameters: {'model_depth': 18, 'learning_rate': 9.022836127243778e-05, 'weight_decay': 3.412943544238458e-05, 'dropout': 0.5252206713395733, 'gamma': False, 'gamma_rate': 0.9371774166078448, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:26:19,795] Trial 179 finished with value: -36.734693877551024 and parameters: {'model_depth': 18, 'learning_rate': 0.00031864103961253167, 'weight_decay': 5.3509186451216166e-05, 'dropout': 0.7474451560413946, 'gamma': True, 'gamma_rate': 0.942510758377423, 'optimizer': 'ASGD'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:26:58,891] Trial 180 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 7.357676209660707e-05, 'weight_decay': 5.337650227505821e-06, 'dropout': 0.1630285914289304, 'gamma': True, 'gamma_rate': 0.9500088542455377, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:27:37,961] Trial 181 finished with value: -73.46938775510205 and parameters: {'model_depth': 18, 'learning_rate': 0.0003754352041629463, 'weight_decay': 8.220822990188101e-06, 'dropout': 0.6458419012520724, 'gamma': True, 'gamma_rate': 0.9510539425727959, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:28:17,026] Trial 182 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 8.023350877160116e-05, 'weight_decay': 9.229689291803275e-06, 'dropout': 0.628754336340643, 'gamma': True, 'gamma_rate': 0.9518825960606946, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:28:56,112] Trial 183 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.000476869483829518, 'weight_decay': 0.0002024622346434194, 'dropout': 0.20726189375268952, 'gamma': True, 'gamma_rate': 0.9587731957645903, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:29:35,200] Trial 184 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 6.744368221533424e-05, 'weight_decay': 9.690852766235526e-06, 'dropout': 0.5812260509488115, 'gamma': True, 'gamma_rate': 0.9546163074521447, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:30:14,242] Trial 185 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 0.00012596940721045654, 'weight_decay': 6.737063684299061e-06, 'dropout': 0.5958567573150167, 'gamma': True, 'gamma_rate': 0.9479950354258371, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:30:53,360] Trial 186 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 9.91525590249066e-05, 'weight_decay': 1.8711364197483312e-05, 'dropout': 0.6662376181344667, 'gamma': True, 'gamma_rate': 0.9522515526206551, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:31:54,085] Trial 187 finished with value: -73.46938775510205 and parameters: {'model_depth': 34, 'learning_rate': 4.811450943695148e-05, 'weight_decay': 4.27396248894095e-06, 'dropout': 0.707680397022266, 'gamma': False, 'gamma_rate': 0.9559894263205021, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:32:33,136] Trial 188 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 8.673529714052455e-05, 'weight_decay': 1.2784108165732798e-05, 'dropout': 0.6381057551645106, 'gamma': True, 'gamma_rate': 0.9494490098921214, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:33:12,233] Trial 189 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 5.49474645471189e-05, 'weight_decay': 0.00010511774982915881, 'dropout': 0.45408249626493624, 'gamma': True, 'gamma_rate': 0.944550321422512, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:33:52,244] Trial 190 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 7.450920015147759e-05, 'weight_decay': 0.0014765537963129644, 'dropout': 0.5044642913229239, 'gamma': True, 'gamma_rate': 0.9611980216516374, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:34:32,423] Trial 191 finished with value: -81.63265306122449 and parameters: {'model_depth': 18, 'learning_rate': 8.891215024085039e-05, 'weight_decay': 5.677392902149026e-06, 'dropout': 0.6608038518228071, 'gamma': True, 'gamma_rate': 0.9519251003987562, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:35:11,704] Trial 192 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 6.212182882124967e-05, 'weight_decay': 6.159968684141424e-06, 'dropout': 0.6844388202695696, 'gamma': True, 'gamma_rate': 0.9540873947284059, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:35:50,973] Trial 193 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 0.00011177229659240049, 'weight_decay': 7.797071311282087e-06, 'dropout': 0.654428335144432, 'gamma': True, 'gamma_rate': 0.9516650522121537, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:36:30,253] Trial 194 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 7.979294224462291e-05, 'weight_decay': 0.00029498017829167014, 'dropout': 0.6162419566273802, 'gamma': True, 'gamma_rate': 0.9535610072674964, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:37:09,517] Trial 195 finished with value: -83.6734693877551 and parameters: {'model_depth': 18, 'learning_rate': 0.00014087613062126935, 'weight_decay': 4.990286433403603e-06, 'dropout': 0.5443863082207161, 'gamma': True, 'gamma_rate': 0.9457722407807575, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:37:48,815] Trial 196 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 0.00014720765835687257, 'weight_decay': 3.463020283018229e-06, 'dropout': 0.5466703749672343, 'gamma': False, 'gamma_rate': 0.9465725530441523, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:38:28,099] Trial 197 finished with value: -79.59183673469387 and parameters: {'model_depth': 18, 'learning_rate': 0.00012931836522719954, 'weight_decay': 2.6452575609671415e-06, 'dropout': 0.527209005566897, 'gamma': True, 'gamma_rate': 0.9412319345405978, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:39:07,704] Trial 198 finished with value: -77.55102040816327 and parameters: {'model_depth': 18, 'learning_rate': 9.994074816792767e-05, 'weight_decay': 4.693846989915896e-06, 'dropout': 0.5605381015675405, 'gamma': True, 'gamma_rate': 0.9437157547872981, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n",
      "[I 2025-02-26 15:39:46,943] Trial 199 finished with value: -75.51020408163265 and parameters: {'model_depth': 18, 'learning_rate': 0.00011918574618512063, 'weight_decay': 9.867905014421327e-06, 'dropout': 0.49438221547772443, 'gamma': True, 'gamma_rate': 0.9482656197519616, 'optimizer': 'Adam'}. Best is trial 25 with value: -85.71428571428571.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial:\n",
      "  Value: -85.71428571428571\n",
      "  Params: \n",
      "    model_depth: 18\n",
      "    learning_rate: 0.00010693471395379046\n",
      "    weight_decay: 2.366898587557215e-06\n",
      "    dropout: 0.4955157772921656\n",
      "    gamma: True\n",
      "    gamma_rate: 0.9401941147532752\n",
      "    optimizer: Adam\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "# train_dataset_path = 'H:/Datasets/int_split/Training/'\n",
    "# val_dataset_path = 'H:/Datasets/int_split/Validation/'\n",
    "# test_dataset_path = 'H:/Datasets/int_split/Testing/'\n",
    "\n",
    "train_dataset_path = 'H:/Datasets/Log_split/Training/'\n",
    "val_dataset_path   = 'H:/Datasets/Log_split/Validation/'\n",
    "test_dataset_path  = 'H:/Datasets/Log_split/Testing/'\n",
    "\n",
    "train_dataset = PTDataset(root_dir=train_dataset_path, target_size=(500, 500))\n",
    "val_dataset = PTDataset(root_dir=val_dataset_path, target_size=(500, 500))\n",
    "test_dataset = PTDataset(root_dir=test_dataset_path, target_size=(500, 500))\n",
    "\n",
    "def get_resnet_model(model_depth=34, dropout=0.5, num_classes=5, input_channels=7):\n",
    "    \"\"\"Returns a ResNet model with specified depth and dropout.\"\"\"\n",
    "    if model_depth == 18:\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    elif model_depth == 34:\n",
    "        model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "    elif model_depth == 50:\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid ResNet depth. Choose from 18, 34, or 50.\")\n",
    "    \n",
    "    model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Modify for 7 channels\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    \"\"\"Evaluates the model on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    avg_loss = running_loss / total\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna optimization function with pruning.\"\"\"\n",
    "    params = {\n",
    "        'model_depth': trial.suggest_categorical('model_depth', [18, 34]),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-2),\n",
    "        'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-2),\n",
    "        'dropout': trial.suggest_uniform('dropout', 0, 0.9),\n",
    "        'gamma': trial.suggest_categorical('gamma', [True, False]),\n",
    "        'gamma_rate': trial.suggest_uniform('gamma_rate', 0.9, 0.99),\n",
    "        'optimizer': trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'ASGD', 'LBFGS'])\n",
    "    }\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = get_resnet_model(params['model_depth'], params['dropout']).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if params['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "    elif params['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'], momentum=0.9)\n",
    "    elif params['optimizer'] == 'ASGD':\n",
    "        optimizer = optim.ASGD(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "    elif params['optimizer'] == 'LBFGS':\n",
    "        optimizer = optim.LBFGS(model.parameters(), lr=params['learning_rate'])  # Removed weight_decay\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer type\")\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=params['gamma_rate']) if params['gamma'] else None\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Training loop with pruning\n",
    "    for epoch in range(25):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            \n",
    "            if params['optimizer'] == 'LBFGS':\n",
    "                optimizer.step(closure)\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Report intermediate result for pruning\n",
    "        trial.report(val_acc, epoch)\n",
    "\n",
    "        # Stop early if performance is worse than previous trials at this step\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return -val_acc  # Minimizing negative accuracy to maximize positive accuracy\n",
    "\n",
    "# Run Optuna study with Median Pruner\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3))\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Display results\n",
    "best_trial = study.best_trial\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Value: {best_trial.value}\")  # Converting back to positive accuracy\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: H:/Datasets/reduced/Training/undetectable\\0_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\0_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\0_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\0_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\0_5.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\0_6.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\100_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\100_6.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\140_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\20_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\20_6.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\20_7.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\30_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\30_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\30_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\40_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\40_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\40_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\40_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\40_5.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\50_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\50_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\50_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\50_5.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\50_6.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\60_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\60_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\60_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\70_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\70_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\70_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\70_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\80_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\80_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\80_5.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\90_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/undetectable\\90_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\1000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\1000_5.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\200_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\200_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\200_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\200_5.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\300_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\300_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\300_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\400_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\400_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\400_5.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\400_6.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\500_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\500_5.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\600_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\600_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\700_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\700_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\700_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\700_5.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\800_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\800_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\800_5.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\900_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\900_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/low\\900_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\10000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\10000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\2000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\2000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\2000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\2000_5.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\3000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\3000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\3000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\4000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\4000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\4000_6.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\4000_8.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\5000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\6000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\6000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\7000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\7000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\8000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\8000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\8000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/medium\\9000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\1000000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\1000000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\1000000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\100000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\100000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\2000000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\2000000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\2000000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\200000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\200000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\200000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\20000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\20000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\20000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\20000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\300000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\30000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\30000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\30000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\30000_7.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\400000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\400000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\40000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\40000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\500000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\500000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\50000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\50000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\50000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\50000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\600000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\600000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\60000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\60000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\60000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\60000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\60000_5.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\700000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\700000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\700000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\700000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\70000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\70000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\800000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\800000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\800000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\80000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\80000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\80000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\900000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\900000_2.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\900000_3.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\900000_4.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\90000_1.pt\n",
      "Saved: H:/Datasets/reduced/Training/high\\90000_2.pt\n",
      "Saved: H:/Datasets/reduced/Validation/undetectable\\100_5.pt\n",
      "Saved: H:/Datasets/reduced/Validation/undetectable\\140_1.pt\n",
      "Saved: H:/Datasets/reduced/Validation/undetectable\\20_1.pt\n",
      "Saved: H:/Datasets/reduced/Validation/undetectable\\20_2.pt\n",
      "Saved: H:/Datasets/reduced/Validation/undetectable\\30_3.pt\n",
      "Saved: H:/Datasets/reduced/Validation/undetectable\\50_4.pt\n",
      "Saved: H:/Datasets/reduced/Validation/undetectable\\80_2.pt\n",
      "Saved: H:/Datasets/reduced/Validation/undetectable\\90_3.pt\n",
      "Saved: H:/Datasets/reduced/Validation/low\\1000_2.pt\n",
      "Saved: H:/Datasets/reduced/Validation/low\\400_4.pt\n",
      "Saved: H:/Datasets/reduced/Validation/low\\500_2.pt\n",
      "Saved: H:/Datasets/reduced/Validation/low\\500_4.pt\n",
      "Saved: H:/Datasets/reduced/Validation/low\\600_1.pt\n",
      "Saved: H:/Datasets/reduced/Validation/low\\600_2.pt\n",
      "Saved: H:/Datasets/reduced/Validation/low\\700_7.pt\n",
      "Saved: H:/Datasets/reduced/Validation/low\\700_8.pt\n",
      "Saved: H:/Datasets/reduced/Validation/low\\800_3.pt\n",
      "Saved: H:/Datasets/reduced/Validation/low\\800_4.pt\n",
      "Saved: H:/Datasets/reduced/Validation/low\\900_2.pt\n",
      "Saved: H:/Datasets/reduced/Validation/low\\900_6.pt\n",
      "Saved: H:/Datasets/reduced/Validation/medium\\10000_5.pt\n",
      "Saved: H:/Datasets/reduced/Validation/medium\\2000_1.pt\n",
      "Saved: H:/Datasets/reduced/Validation/medium\\2000_6.pt\n",
      "Saved: H:/Datasets/reduced/Validation/medium\\4000_2.pt\n",
      "Saved: H:/Datasets/reduced/Validation/medium\\5000_1.pt\n",
      "Saved: H:/Datasets/reduced/Validation/medium\\5000_4.pt\n",
      "Saved: H:/Datasets/reduced/Validation/medium\\6000_2.pt\n",
      "Saved: H:/Datasets/reduced/Validation/medium\\7000_2.pt\n",
      "Saved: H:/Datasets/reduced/Validation/medium\\6000_4.pt\n",
      "Saved: H:/Datasets/reduced/Validation/medium\\9000_2.pt\n",
      "Saved: H:/Datasets/reduced/Validation/medium\\10000_3.pt\n",
      "Saved: H:/Datasets/reduced/Validation/medium\\4000_5.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\100000_2.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\100000_6.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\2000000_3.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\300000_2.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\30000_1.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\30000_5.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\400000_1.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\400000_4.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\40000_3.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\500000_3.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\500000_4.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\600000_4.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\60000_6.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\70000_5.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\70000_6.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\90000_3.pt\n",
      "Saved: H:/Datasets/reduced/Validation/high\\90000_4.pt\n",
      "Saved: H:/Datasets/reduced/Testing/undetectable\\100_2.pt\n",
      "Saved: H:/Datasets/reduced/Testing/undetectable\\100_3.pt\n",
      "Saved: H:/Datasets/reduced/Testing/undetectable\\100_4.pt\n",
      "Saved: H:/Datasets/reduced/Testing/undetectable\\20_5.pt\n",
      "Saved: H:/Datasets/reduced/Testing/undetectable\\60_4.pt\n",
      "Saved: H:/Datasets/reduced/Testing/undetectable\\80_3.pt\n",
      "Saved: H:/Datasets/reduced/Testing/undetectable\\90_1.pt\n",
      "Saved: H:/Datasets/reduced/Testing/undetectable\\20_4.pt\n",
      "Saved: H:/Datasets/reduced/Testing/low\\1000_3.pt\n",
      "Saved: H:/Datasets/reduced/Testing/low\\200_4.pt\n",
      "Saved: H:/Datasets/reduced/Testing/low\\300_3.pt\n",
      "Saved: H:/Datasets/reduced/Testing/low\\400_1.pt\n",
      "Saved: H:/Datasets/reduced/Testing/low\\500_3.pt\n",
      "Saved: H:/Datasets/reduced/Testing/low\\500_6.pt\n",
      "Saved: H:/Datasets/reduced/Testing/low\\600_5.pt\n",
      "Saved: H:/Datasets/reduced/Testing/low\\700_3.pt\n",
      "Saved: H:/Datasets/reduced/Testing/low\\700_6.pt\n",
      "Saved: H:/Datasets/reduced/Testing/low\\800_6.pt\n",
      "Saved: H:/Datasets/reduced/Testing/low\\900_5.pt\n",
      "Saved: H:/Datasets/reduced/Testing/low\\600_6.pt\n",
      "Saved: H:/Datasets/reduced/Testing/low\\1000_4.pt\n",
      "Saved: H:/Datasets/reduced/Testing/medium\\10000_2.pt\n",
      "Saved: H:/Datasets/reduced/Testing/medium\\3000_3.pt\n",
      "Saved: H:/Datasets/reduced/Testing/medium\\3000_5.pt\n",
      "Saved: H:/Datasets/reduced/Testing/medium\\4000_4.pt\n",
      "Saved: H:/Datasets/reduced/Testing/medium\\4000_7.pt\n",
      "Saved: H:/Datasets/reduced/Testing/medium\\5000_2.pt\n",
      "Saved: H:/Datasets/reduced/Testing/medium\\5000_5.pt\n",
      "Saved: H:/Datasets/reduced/Testing/medium\\5000_6.pt\n",
      "Saved: H:/Datasets/reduced/Testing/medium\\7000_1.pt\n",
      "Saved: H:/Datasets/reduced/Testing/medium\\8000_1.pt\n",
      "Saved: H:/Datasets/reduced/Testing/medium\\9000_1.pt\n",
      "Saved: H:/Datasets/reduced/Testing/medium\\9000_4.pt\n",
      "Saved: H:/Datasets/reduced/Testing/high\\1000000_2.pt\n",
      "Saved: H:/Datasets/reduced/Testing/high\\100000_4.pt\n",
      "Saved: H:/Datasets/reduced/Testing/high\\100000_5.pt\n",
      "Saved: H:/Datasets/reduced/Testing/high\\200000_4.pt\n",
      "Saved: H:/Datasets/reduced/Testing/high\\300000_1.pt\n",
      "Saved: H:/Datasets/reduced/Testing/high\\300000_3.pt\n",
      "Saved: H:/Datasets/reduced/Testing/high\\30000_6.pt\n",
      "Saved: H:/Datasets/reduced/Testing/high\\40000_4.pt\n",
      "Saved: H:/Datasets/reduced/Testing/high\\600000_3.pt\n",
      "Saved: H:/Datasets/reduced/Testing/high\\70000_3.pt\n",
      "Saved: H:/Datasets/reduced/Testing/high\\70000_4.pt\n",
      "Saved: H:/Datasets/reduced/Testing/high\\800000_4.pt\n",
      "Saved: H:/Datasets/reduced/Testing/high\\80000_2.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def process_and_save(root_dir, save_dir, target_size=(500, 500)):\n",
    "    \"\"\"\n",
    "    Extracts the first 20 frames' average and 6 selected frames, resizes, and saves to new .pt files.\n",
    "    \"\"\"\n",
    "    classes = ['undetectable', 'low', 'medium', 'high']\n",
    "    selected_frame_indices = [69, 89, 109, 129, 149, 179]\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    for label in classes:\n",
    "        class_path = os.path.join(root_dir, label)\n",
    "        save_class_path = os.path.join(save_dir, label)\n",
    "        \n",
    "        if not os.path.exists(class_path):\n",
    "            continue  # Skip if folder doesn't exist\n",
    "        \n",
    "        if not os.path.exists(save_class_path):\n",
    "            os.makedirs(save_class_path)\n",
    "        \n",
    "        for file in os.listdir(class_path):\n",
    "            if file.endswith('.pt'):\n",
    "                file_path = os.path.join(class_path, file)\n",
    "                save_file_path = os.path.join(save_class_path, file)\n",
    "                \n",
    "                \n",
    "                tensor_data = torch.load(file_path, map_location='cpu')  # [C, T, H, W]\n",
    "                max_frames = tensor_data.shape[1]\n",
    "                \n",
    "                \n",
    "                valid_frames = [i for i in selected_frame_indices if i < max_frames]\n",
    "                if len(valid_frames) < 6:\n",
    "                    print(f\"Skipping {file_path}: Not enough frames ({max_frames} available, required 180)\")\n",
    "                    continue\n",
    "                \n",
    "                # Compute average of the first 20 frames\n",
    "                avg_first_20 = torch.mean(tensor_data[:, :20, :, :], dim=1, keepdim=True)  # [C, 1, H, W]\n",
    "                selected_frames = tensor_data[:, valid_frames, :, :]  # [C, 6, H, W]\n",
    "                \n",
    "                # Concatenate to form a 7-frame tensor\n",
    "                final_tensor = torch.cat((avg_first_20, selected_frames), dim=1)  # [C, 7, H, W]\n",
    "                final_tensor = final_tensor.squeeze(0) if final_tensor.shape[0] == 1 else final_tensor\n",
    "                \n",
    "              \n",
    "                if final_tensor.dim() == 3:\n",
    "                    final_tensor = final_tensor.unsqueeze(0)  # -> [1, 7, H, W]\n",
    "                \n",
    "                resized_tensor = F.interpolate(\n",
    "                    final_tensor, size=target_size, mode='bilinear', align_corners=False\n",
    "                )\n",
    "                \n",
    "            \n",
    "                torch.save(resized_tensor, save_file_path)\n",
    "                print(f\"Saved: {save_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "train_dataset_path = 'H:/Datasets/int_split/Training/'\n",
    "val_dataset_path = 'H:/Datasets/int_split/Validation/'\n",
    "test_dataset_path = 'H:/Datasets/int_split/Testing/'\n",
    "\n",
    "train_save_path = 'H:/Datasets/reduced/Training/'\n",
    "val_save_path = 'H:/Datasets/reduced/Validation/'\n",
    "test_save_path = 'H:/Datasets/reduced/Testing/'\n",
    "\n",
    "process_and_save(train_dataset_path, train_save_path)\n",
    "process_and_save(val_dataset_path, val_save_path)\n",
    "process_and_save(test_dataset_path, test_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
