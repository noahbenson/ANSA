{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Naming Convention\n",
    "\n",
    "Each `.pt` file should follow this format:\n",
    "\n",
    "```\n",
    "<label>_<replicate>.pt\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "- `150_1.pt` → Label: 150 (undetectable), Replicate: 1\n",
    "- `500_2.pt` → Label: 500 (low), Replicate: 2\n",
    "- `7000_3.pt` → Label: 7000 (medium), Replicate: 3\n",
    "- `20000_5.pt` → Label: 20000 (high), Replicate: 5\n",
    "\n",
    "\n",
    "\n",
    "## Class Definitions for Semi-Quantitative approach \n",
    "\n",
    "Infers clinical decision-making based on viral load counts (assuming 1:1 sample prep\n",
    ")\n",
    "1. `undetectable` → Label values `< 200`\n",
    "2. `low` → Label values `200 ≤ label ≤ 1000`\n",
    "3. `medium` → Label values `1000 < label ≤ 10000`\n",
    "4. `high` → Label values `> 10000`\n",
    "\n",
    "# Dataset Folder Structure\n",
    "\n",
    "Dataset is organized into the following structure to ensure proper training, validation, and testing:\n",
    "\n",
    "```\n",
    "Datasets/\n",
    "│-- SemiQuant/\n",
    "│   │-- Training/             # Training dataset (60% of total data)\n",
    "│   │   ├── undetectable/      # Class 0 (e.g., files with labels < 200)\n",
    "│   │   │   ├── 20_1.pt\n",
    "│   │   │   ├── 40_3.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── low/               # Class 1 (200 ≤ label ≤ 1000)\n",
    "|   |   |   ├── 300_2.pt\n",
    "│   │   │   ├── 600_4.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── medium/            # Class 2 (1000 < label ≤ 10000)\n",
    "│   │   │   ├── 2000_1.pt\n",
    "│   │   │   ├── 7000_2.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── high/              # Class 3 (label > 10000)\n",
    "│   │   │   ├── 10000_2.pt\n",
    "│   │   │   ├── 90000_2.pt\n",
    "│   │   │   └── ...\n",
    "│\n",
    "│   │-- Validation/            # Validation dataset (20% of total data)\n",
    "│   │   ├── undetectable/\n",
    "│   │   │   ├── 20_2.pt\n",
    "│   │   │   ├── 40_4.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── low/\n",
    "│   │   ├── medium/\n",
    "│   │   ├── high/\n",
    "│\n",
    "│   │-- Testing/               # Testing dataset (20% of total data)\n",
    "│   │   ├── undetectable/\n",
    "│   │   │   ├── 30_1.pt\n",
    "│   │   │   ├── 50_2.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── low/\n",
    "│   │   ├── medium/\n",
    "│   │   ├── high/\n",
    "│\n",
    "│-- torch_tensors/              # Original .pt files before splitting\n",
    "│   │   ├── 100_1.pt\n",
    "│   │   ├── 200_3.pt\n",
    "│   │   ├── 5000_2.pt\n",
    "│   │   ├── 15000_4.pt\n",
    "│   │   └── ...\n",
    "```\n",
    "\n",
    "## Folder Descriptions\n",
    "\n",
    "- **`Training/`** – Used to train the model (60% of total data).\n",
    "- **`Validation/`** – Used to validate the model during training (20% of total data).\n",
    "- **`Testing/`** – Used to evaluate the model after training (20% of total data).\n",
    "- **`torch_tensors/`** – Stores the original `.pt` files before they were split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTDataset(Dataset):\n",
    "    def __init__(self, root_dir, target_size=(500, 500), transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the dataset directory (e.g., Training folder).\n",
    "            target_size (tuple): Desired output size (height, width).\n",
    "            transform (callable, optional): Optional transformations (on CPU).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "        self.classes = ['undetectable', 'low', 'medium', 'high']\n",
    "\n",
    "        # Collect all file paths and labels\n",
    "        self.file_list = []\n",
    "        for label in self.classes:\n",
    "            class_path = os.path.join(root_dir, label)\n",
    "            if not os.path.exists(class_path):\n",
    "                continue  # Skip if folder doesn't exist\n",
    "            for file in os.listdir(class_path):\n",
    "                if file.endswith('.pt'):\n",
    "                    full_path = os.path.join(class_path, file)\n",
    "                    class_index = self.classes.index(label)\n",
    "                    self.file_list.append((full_path, class_index))\n",
    "\n",
    "        # Pre-load everything into memory (CPU)\n",
    "        self.data_list = []\n",
    "        for file_path, label in self.file_list:\n",
    "            # Load from disk to CPU memory\n",
    "            tensor_data = torch.load(file_path, map_location='cpu')  # [C, T, H, W]\n",
    "\n",
    "            # Ensure enough frames\n",
    "            max_frames = tensor_data.shape[1]\n",
    "            selected_frame_indices = [69, 89, 109, 129, 149, 179]\n",
    "            selected_frame_indices = [i for i in selected_frame_indices if i < max_frames]\n",
    "            if len(selected_frame_indices) < 6:\n",
    "                raise ValueError(f\"Not enough frames in {file_path}, available: {max_frames}, required: 180\")\n",
    "\n",
    "            # Compute average of the first 20 frames\n",
    "            avg_first_20 = torch.mean(tensor_data[:, :20, :, :], dim=1, keepdim=True)  # [C, 1, H, W]\n",
    "            selected_frames = tensor_data[:, selected_frame_indices, :, :]             # [C, 6, H, W]\n",
    "\n",
    "            # Concatenate to form a 7-frame tensor\n",
    "            final_tensor = torch.cat((avg_first_20, selected_frames), dim=1)  # [C, 7, H, W]\n",
    "            final_tensor = final_tensor.squeeze(0) if final_tensor.shape[0] == 1 else final_tensor\n",
    "\n",
    "            # Resize on CPU\n",
    "            if final_tensor.dim() == 3:\n",
    "                # shape [7, H, W]\n",
    "                final_tensor = final_tensor.unsqueeze(0)  # -> [1, 7, H, W]\n",
    "\n",
    "            resized_tensor = F.interpolate(\n",
    "                final_tensor,\n",
    "                size=self.target_size,\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            # Optional transform\n",
    "            if self.transform:\n",
    "                resized_tensor = self.transform(resized_tensor)\n",
    "\n",
    "            # Model expects input_channels=7, flatten [C=1, frames=7, H, W] -> [7, H, W]\n",
    "            if resized_tensor.shape[0] == 1:\n",
    "                resized_tensor = resized_tensor.squeeze(0)  # shape [7, H, W]\n",
    "\n",
    "            # Store (tensor, label)\n",
    "            self.data_list.append((resized_tensor, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_model(num_classes=4, input_channels=7):\n",
    "    \"\"\"\n",
    "    Build ResNet34 with a custom first conv layer\n",
    "    that expects `input_channels`.\n",
    "    \"\"\"\n",
    "    model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "    # Replace first conv to match your input_channels\n",
    "    model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    # Replace FC layer to match num_classes\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Basic training routine using CrossEntropyLoss\n",
    "    for single-label, multi-class classification.\n",
    "    \"\"\"\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = 100.0 * correct / total\n",
    "\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def plot_confusion_matrix(model, loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Generates and displays a confusion matrix for the model on the given loader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. Check device for cuda or cpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 2. Define dataset paths\n",
    "    train_dataset_path = 'M:/Datasets/int_split/Training/'\n",
    "    val_dataset_path = 'M:/Datasets/int_split/Validation/'\n",
    "    test_dataset_path = 'M:/Datasets/int_split/Testing/'\n",
    "\n",
    "    # 3. Load datasets\n",
    "    train_dataset = PTDataset(root_dir=train_dataset_path, target_size=(500, 500))\n",
    "    val_dataset = PTDataset(root_dir=val_dataset_path, target_size=(500, 500))\n",
    "    test_dataset = PTDataset(root_dir=test_dataset_path, target_size=(500, 500))\n",
    "\n",
    "    # 4. Create WeightedRandomSampler for training\n",
    "    train_labels = [label for _, label in train_dataset.data_list]\n",
    "    class_counts = Counter(train_labels)\n",
    "    weights = [1.0 / class_counts[label] for label in train_labels]\n",
    "\n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=weights,\n",
    "        num_samples=len(weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    # 5. Create DataLoaders\n",
    "    use_pin_memory = True if device.type == 'cuda' else False\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        sampler=train_sampler,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    # 6. Build model, move to device\n",
    "    model = get_resnet_model(num_classes=4, input_channels=7)\n",
    "    model.to(device)\n",
    "\n",
    "    # 7. Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "\n",
    "    # 8. Train\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=13)\n",
    "\n",
    "    # 9. Evaluate on test set\n",
    "    test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    # 10. Save model\n",
    "    torch.save(model.state_dict(), 'resnet_model_gamma_13.pth')\n",
    "    print(\"Model saved as resnet_model.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "#     # If you want to plot the confusion matrix after everything is done:\n",
    "#     # (Just call the function from anywhere outside `main()`.)\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     class_names = ['Und.', 'Low', 'Med', 'High']\n",
    "\n",
    "#     # Re-initialize your model, load state dict if needed, etc.\n",
    "#     model = get_resnet_model(num_classes=4, input_channels=7)\n",
    "#     model.load_state_dict(torch.load('resnet_model_gamma2.pth', map_location=device))\n",
    "#     model.to(device)\n",
    "\n",
    "    # # Re-create test_loader (or pass it around as a global var) # ran out of ram for this to be in the funct\n",
    "    # test_dataset_path = 'M:/Datasets/int_split/Testing/'\n",
    "    # test_dataset = PTDataset(root_dir=test_dataset_path, target_size=(500, 500))\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    # plot_confusion_matrix(model, test_loader, device, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(model, test_loader, device, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to appropriate device (CPU/GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Collect all true labels and predictions\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:  \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy()) \n",
    "        all_labels.extend(labels.cpu().numpy())  \n",
    "\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "class_names = ['Und.', 'Low', 'Med', 'High']  \n",
    "\n",
    "\n",
    "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[7][1])\n",
    "plt.imshow(train_dataset[7][0][-1,:,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANSA_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
