{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage.io import imread\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import shutil\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\0_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\0_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\0_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\0_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\0_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\0_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\1000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\1000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\1000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\1000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\1000_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\500_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\500_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\500_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\500_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\500_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\500_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\200_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\200_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\200_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\200_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\200_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\80_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\80_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\80_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\80_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\80_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\40_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\40_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\40_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\40_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\40_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\20_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\20_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\20_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\20_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\20_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\20_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\20_7.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\30000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\30000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\30000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\30000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\30000_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\30000_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\30000_7.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\10000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\10000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\10000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\10000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\10000_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\8000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\8000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\8000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\8000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\5000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\5000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\5000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\5000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\5000_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\5000_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\3000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\3000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\3000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\3000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\3000_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\2000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\2000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\2000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\2000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\2000_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\2000_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\50000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\50000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\50000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\50000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\80000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\80000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\80000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\80000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\100000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\100000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\100000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\100000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\100000_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\100000_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\90000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\90000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\90000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\90000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\40000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\40000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\40000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\40000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\20000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\20000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\20000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\20000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\9000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\9000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\9000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\9000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\4000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\4000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\4000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\4000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\4000_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\4000_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\4000_7.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\4000_8.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\30_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\30_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\30_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\30_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\90_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\90_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\90_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\90_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\200000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\200000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\200000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\200000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\300000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\300000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\300000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\300000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\500000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\500000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\500000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\500000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\400000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\400000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\400000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\400000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\800000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\800000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\800000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\800000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\600000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\600000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\600000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\600000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\700000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\700000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\700000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\700000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\900000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\900000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\900000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\900000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\1000000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\1000000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\1000000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\1000000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\50_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\50_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\50_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\50_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\50_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\50_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\70_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\70_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\70_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\70_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\900_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\900_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\900_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\900_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\900_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\900_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\400_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\400_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\400_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\400_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\400_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\400_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\700_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\700_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\700_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\700_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\700_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\700_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\700_7.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\700_8.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\800_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\800_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\800_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\800_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\800_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\800_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\600_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\600_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\600_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\600_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\600_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\600_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\60_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\60_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\60_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\60_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\300_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\300_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\300_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\300_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\100_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\100_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\100_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\100_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\100_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\100_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\2000000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\2000000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\2000000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\2000000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\70000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\70000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\70000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\70000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\70000_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\70000_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\60000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\60000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\60000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\60000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\60000_5.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\60000_6.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\7000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\7000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\7000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\7000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\6000_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\6000_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\6000_3.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\6000_4.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\140_1.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Processed: H:\\Datasetsinterpolated_torch_tensors\\140_2.pt -> New shape: torch.Size([1, 180, 500, 500])\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "input_dir = \"G:/Shared drives/Posner Group Current/Cole's Files/ANSA/RPA on glass slides/100_serial/processed\"\n",
    "output_dir = \"H:\\Datasetsinterpolated_torch_tensors\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to get all image paths and their labels\n",
    "def get_image_paths_and_labels(root_dir):\n",
    "    image_data = []\n",
    "    for label in os.listdir(root_dir):\n",
    "        label_path = os.path.join(root_dir, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for idx, image_name in enumerate(os.listdir(label_path)):\n",
    "                if image_name.lower().endswith('.tif'):\n",
    "                    image_path = os.path.join(label_path, image_name)\n",
    "                    image_data.append((image_path, f\"{label}_{idx+1}\"))\n",
    "    return image_data\n",
    "\n",
    "# Get all image paths and labels\n",
    "image_data = get_image_paths_and_labels(input_dir)\n",
    "\n",
    "# Upsample/interpolate to 500x500\n",
    "target_shape = (1, 180, 500, 500)\n",
    "\n",
    "for img_path, label in image_data:\n",
    "    # Load the image stack\n",
    "    image_stack = imread(img_path)\n",
    "    \n",
    "    # Convert to tensor and add channel dimension [f, h, w] -> [1, f, h, w]\n",
    "    tensor = torch.tensor(image_stack, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # Upsample to target size using bicubic interpolation\n",
    "    upsampled_tensor = F.interpolate(\n",
    "        tensor, \n",
    "        size=(500,500), \n",
    "        mode='bicubic', \n",
    "        align_corners=True\n",
    "    )\n",
    "\n",
    "    # Save with corrected filename\n",
    "    output_path = os.path.join(output_dir, f\"{label}.pt\")\n",
    "    torch.save(upsampled_tensor, output_path)\n",
    "    \n",
    "    print(f\"Processed: {output_path} -> New shape: {upsampled_tensor.shape}\")\n",
    "\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Repadding to make them square ([1,180,499,500] currently)\n",
    "# input_folder = r'../Datasets/torch_tensors/'\n",
    "# output_folder = input_folder  # Change if you want to save elsewhere\n",
    "\n",
    "# # os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # Desired shape\n",
    "# target_shape = (1, 180, 500, 500)\n",
    "\n",
    "# # Process each tensor file\n",
    "# for file_name in os.listdir(input_folder):\n",
    "#     if file_name.endswith('.pt'):\n",
    "#         file_path = os.path.join(input_folder, file_name)\n",
    "        \n",
    "#         tensor = torch.load(file_path)\n",
    "\n",
    "#         _, _, current_height, current_width = tensor.shape\n",
    "        \n",
    "#         # Calculate padding amounts (last two dimensions: width and height)\n",
    "#         pad_width = target_shape[-1] - current_width  # Padding for width (500 - 499)\n",
    "#         pad_height = target_shape[-2] - current_height  # Padding for height (should be 0)\n",
    "\n",
    "#         # Apply padding (pad format: last dimension first, so width then height)\n",
    "#         padded_tensor = torch.nn.functional.pad(tensor, (0, pad_width, 0, pad_height), \"constant\", 0)\n",
    "\n",
    "#         output_path = os.path.join(output_folder, file_name)\n",
    "#         torch.save(padded_tensor, output_path)\n",
    "\n",
    "#         print(f\"Processed: {file_name} -> New shape: {padded_tensor.shape}\")\n",
    "\n",
    "# print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle done\n",
      "Train copy done\n",
      "Test copy done\n",
      "Validation copy done\n",
      "Copied 141 files to Training\n",
      "Copied 47 files to Testing\n",
      "Copied 48 files to Validation\n"
     ]
    }
   ],
   "source": [
    "source_dir = 'H:/Datasets/interpolated_torch_tensors'\n",
    "output_dir = 'H:/Datasets/int_Split'\n",
    "\n",
    "random.seed(53)\n",
    "\n",
    "train_dir = os.path.join(output_dir, 'Training')\n",
    "test_dir = os.path.join(output_dir, 'Testing')\n",
    "val_dir = os.path.join(output_dir, 'Validation')\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "all_files = [f for f in os.listdir(source_dir) if f.endswith('.pt')]\n",
    "\n",
    "random.shuffle(all_files)\n",
    "print(\"Shuffle done\")\n",
    "num_files = len(all_files)\n",
    "train_split = int(num_files * 0.60)\n",
    "test_split = int(num_files * 0.20)\n",
    "\n",
    "train_files = all_files[:train_split]\n",
    "test_files = all_files[train_split:train_split + test_split]\n",
    "val_files = all_files[train_split + test_split:]  # Remainder goes here\n",
    "\n",
    "def copy_files(file_list, dest_dir):\n",
    "    for file in file_list:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, file))\n",
    "\n",
    "copy_files(train_files, train_dir)\n",
    "print(\"Train copy done\")\n",
    "\n",
    "copy_files(test_files, test_dir)\n",
    "print(\"Test copy done\")\n",
    "\n",
    "copy_files(val_files, val_dir)\n",
    "print(\"Validation copy done\")\n",
    "\n",
    "\n",
    "print(f\"Copied {len(train_files)} files to Training\")\n",
    "print(f\"Copied {len(test_files)} files to Testing\")\n",
    "print(f\"Copied {len(val_files)} files to Validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been copied to respective categories:\n",
      "undetectable: 7 files\n",
      "low: 11 files\n",
      "medium: 16 files\n",
      "high: 13 files\n",
      "Files have been copied to respective categories:\n",
      "undetectable: 37 files\n",
      "low: 27 files\n",
      "medium: 22 files\n",
      "high: 55 files\n",
      "Files have been copied to respective categories:\n",
      "undetectable: 9 files\n",
      "low: 14 files\n",
      "medium: 8 files\n",
      "high: 17 files\n"
     ]
    }
   ],
   "source": [
    "# Define training directory and output category folders\n",
    "Paths = ['H:/Datasets/int_Split/Testing/', 'H:/Datasets/int_Split/Training/',\n",
    "  'H:/Datasets/int_Split/Validation/']\n",
    "\n",
    "for Path in Paths:\n",
    "    train_dir = Path\n",
    "    category_dirs = {\n",
    "        'undetectable': os.path.join(train_dir, 'undetectable'),\n",
    "        'low': os.path.join(train_dir, 'low'),\n",
    "        'medium': os.path.join(train_dir, 'medium'),\n",
    "        'high': os.path.join(train_dir, 'high')\n",
    "    }\n",
    "\n",
    "    # Create category directories if they don't exist\n",
    "    for dir_path in category_dirs.values():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # Function to categorize based on label value\n",
    "    def categorize_file(filename):\n",
    "        try:\n",
    "            label = int(filename.split('_')[0])  # Extract numeric label before underscore\n",
    "            if label < 200:\n",
    "                return 'undetectable'\n",
    "            elif 200 <= label <= 1000:\n",
    "                return 'low'\n",
    "            elif 1000 < label <= 10000:\n",
    "                return 'medium'\n",
    "            else:\n",
    "                return 'high'\n",
    "        except ValueError:\n",
    "            print(f\"Skipping {filename}: Invalid format\")\n",
    "            return None\n",
    "\n",
    "    # Process each file in the training directory\n",
    "    for file in os.listdir(train_dir):\n",
    "        if file.endswith('.pt'):\n",
    "            category = categorize_file(file)\n",
    "            if category:\n",
    "                src_path = os.path.join(train_dir, file)\n",
    "                dest_path = os.path.join(category_dirs[category], file)\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "    print(\"Files have been copied to respective categories:\")\n",
    "    for category, path in category_dirs.items():\n",
    "        print(f\"{category}: {len(os.listdir(path))} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANSA_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
