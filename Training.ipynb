{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTDataset(Dataset):\n",
    "    def __init__(self, root_dir, target_size=(500, 500), transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the dataset directory (e.g., Training folder).\n",
    "            target_size (tuple): Desired output size (height, width).\n",
    "            transform (callable, optional): Optional transformations (on CPU).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "        self.classes = ['undetectable', 'low', 'medium', 'high']\n",
    "\n",
    "        # Collect all file paths and labels\n",
    "        self.file_list = []\n",
    "        for label in self.classes:\n",
    "            class_path = os.path.join(root_dir, label)\n",
    "            if not os.path.exists(class_path):\n",
    "                continue  # Skip if folder doesn't exist\n",
    "            for file in os.listdir(class_path):\n",
    "                if file.endswith('.pt'):\n",
    "                    full_path = os.path.join(class_path, file)\n",
    "                    class_index = self.classes.index(label)\n",
    "                    self.file_list.append((full_path, class_index))\n",
    "\n",
    "        # Pre-load everything into memory (CPU)\n",
    "        self.data_list = []\n",
    "        for file_path, label in self.file_list:\n",
    "            # Load from disk to CPU memory\n",
    "            tensor_data = torch.load(file_path, map_location='cpu')  # [7, H, W]\n",
    "\n",
    "            # Resize on CPU\n",
    "            if tensor_data.dim() == 3:\n",
    "                tensor_data = tensor_data.unsqueeze(0)  # -> [1, 7, H, W]\n",
    "\n",
    "            resized_tensor = F.interpolate(\n",
    "                tensor_data,\n",
    "                size=self.target_size,\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            # Optional transform\n",
    "            if self.transform:\n",
    "                resized_tensor = self.transform(resized_tensor)\n",
    "\n",
    "            # Model expects input_channels=7, flatten [C=1, 7, H, W] -> [7, H, W]\n",
    "            if resized_tensor.shape[0] == 1:\n",
    "                resized_tensor = resized_tensor.squeeze(0)  # shape [7, H, W]\n",
    "\n",
    "            # Store (tensor, label)\n",
    "            self.data_list.append((resized_tensor, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Labels in dataset: {0, 1, 2, 3}\n",
      "\n",
      "=== Training Run 1 of 5 ===\n",
      "Epoch [1/18] Train Loss: 1.2389, Train Acc: 42.55%, Val Loss: 1.4600, Val Acc: 20.41%\n",
      "Epoch [2/18] Train Loss: 0.7317, Train Acc: 75.18%, Val Loss: 1.0365, Val Acc: 63.27%\n",
      "Epoch [3/18] Train Loss: 0.3552, Train Acc: 86.52%, Val Loss: 0.6746, Val Acc: 73.47%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 213\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% for the best model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 213\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 179\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    175\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mExponentialLR(optimizer\u001b[38;5;241m=\u001b[39moptimizer, gamma\u001b[38;5;241m=\u001b[39mgamma_rate)\n\u001b[0;32m    178\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m18\u001b[39m\n\u001b[1;32m--> 179\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[0;32m    191\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_loader, criterion, device)\n",
      "Cell \u001b[1;32mIn[3], line 72\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[0;32m     68\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     71\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 72\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     75\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Colem\\anaconda3\\envs\\ANSA_ML\\lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\Colem\\anaconda3\\envs\\ANSA_ML\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\Colem\\anaconda3\\envs\\ANSA_ML\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_resnet_model(num_classes=4, input_channels=7, dropout_rate=0.1454303215712593):\n",
    "    \"\"\"\n",
    "    Build ResNet18 with a custom first conv layer\n",
    "    that expects `input_channels` and adds a Dropout layer.\n",
    "\n",
    "    model_depth = 18 (ResNet18)\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    \n",
    "    # Replace first conv to match your input_channels\n",
    "    model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    \n",
    "    # Replace FC layer to include Dropout before classification\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_rate),  # Dropout before final classification\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            # Use AMP if on GPU\n",
    "            with torch.amp.autocast(device_type='cuda', enabled=(device.type == 'cuda')):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=25):\n",
    "    \"\"\"\n",
    "    Basic training routine using CrossEntropyLoss\n",
    "    for single-label, multi-class classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            with torch.amp.autocast(device_type='cuda', enabled=(device.type == 'cuda')):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = 100.0 * correct / total\n",
    "\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "train_dataset_path = 'H:/Datasets/reduced/Training/'\n",
    "val_dataset_path   = 'H:/Datasets/reduced/Validation/'\n",
    "test_dataset_path  = 'H:/Datasets/reduced/Testing/'\n",
    "\n",
    "\n",
    "def main():\n",
    "    #Check device for CUDA or CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    \n",
    "    train_dataset = PTDataset(root_dir=train_dataset_path, target_size=(500, 500))\n",
    "    val_dataset   = PTDataset(root_dir=val_dataset_path,   target_size=(500, 500))\n",
    "    test_dataset  = PTDataset(root_dir=test_dataset_path,  target_size=(500, 500))\n",
    "\n",
    "   \n",
    "    train_labels = [label for _, label in train_dataset.data_list]\n",
    "    print(\"Labels in dataset:\", set(train_labels))\n",
    "\n",
    "    class_counts = Counter(train_labels)\n",
    "    weights = [1.0 / class_counts[label] for label in train_labels]\n",
    "    \n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=weights,\n",
    "        num_samples=len(weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    use_pin_memory = (device.type == 'cuda')\n",
    "    batch_size = 32 \n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=use_pin_memory\n",
    "    )\n",
    "\n",
    "    \n",
    "    best_val_acc = -float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    # Train/Evaluate 5 times\n",
    "    for run_idx in range(5):\n",
    "        print(f\"\\n=== Training Run {run_idx+1} of 5 ===\")\n",
    "\n",
    "        \n",
    "        model = get_resnet_model(num_classes=4, input_channels=7)  \n",
    "        model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        learning_rate = 0.0038825206157311557\n",
    "        weight_decay  = 0.0001931053552153856\n",
    "        gamma_rate    = 0.9388047294838997\n",
    "        \n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            momentum=0.9\n",
    "        )\n",
    "\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=gamma_rate)\n",
    "\n",
    "        \n",
    "        num_epochs = 18\n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            num_epochs=num_epochs\n",
    "        )\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        print(f\"Run {run_idx+1} validation accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        # Keep track of best model so far\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"New best model found with val_acc={val_acc:.2f}% (Run {run_idx+1}).\")\n",
    "\n",
    "    # After all 5 runs, save only the best model\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, 'resnet_model_Docker.pth')\n",
    "        print(f\"\\nBest model saved with val_acc={best_val_acc:.2f}%\")\n",
    "\n",
    "        best_model = get_resnet_model(num_classes=4, input_channels=7)\n",
    "        best_model.load_state_dict(best_model_state)\n",
    "        best_model.to(device)\n",
    "\n",
    "        test_loss, test_acc = evaluate_model(best_model, test_loader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}% for the best model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANSA_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
